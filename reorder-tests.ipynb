{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pymupdf easyocr opencv-python numpy scikit-learn sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h91JJxYt-qkk",
        "outputId": "055d39c6-692d-4a20-e137-e7ee80ec140e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.1)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.8.28)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SvpaMhj69eT",
        "outputId": "7ca2ed25-825a-40da-a328-bb759110d329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.1)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "🚀 Starting STEP-BY-STEP PDF reordering...\n",
            "🔍 Analyzing 25 pages - step by step approach...\n",
            "📄 Page 1\n",
            "📄 Page 2\n",
            "📄 Page 3\n",
            "📄 Page 4\n",
            "📄 Page 5\n",
            "📄 Page 6\n",
            "📄 Page 7\n",
            "📄 Page 8\n",
            "📄 Page 9\n",
            "📄 Page 10\n",
            "📄 Page 11\n",
            "📄 Page 12\n",
            "📄 Page 13\n",
            "📄 Page 14\n",
            "📄 Page 15\n",
            "📄 Page 16\n",
            "📄 Page 17\n",
            "📄 Page 18\n",
            "📄 Page 19\n",
            "📄 Page 20\n",
            "📄 Page 21\n",
            "📄 Page 22\n",
            "📄 Page 23\n",
            "📄 Page 24\n",
            "📄 Page 25\n",
            "\n",
            "📋 STEP 1: Grouping pages by type...\n",
            "   schedules: 12 pages\n",
            "      Page 4: LOANAGREEMENT No. Dated (Project No: Borrower Ms. ...\n",
            "      Page 8: 22- ARTICLE : ! DEFINTIONS GENERAL CONDITIONS 1.1 ...\n",
            "      Page 9: 5- In the event of the Borrower, failing to pay th...\n",
            "   articles: 2 pages\n",
            "      Page 1: 7- ARTICLE - IV APPOINTMENT QE NOMNEE DIRECTORS Th...\n",
            "      Page 6: -9 ARTICLE VI EFFECTIVE DATE QE AGREEMENTL PLACE Q...\n",
            "   numbered_pages: 6 pages\n",
            "      Page 7: -16- viii) The Borrower  shall at its own cost kee...\n",
            "      Page 15: -21 - D) CONDITIONS APPLICABLE TO LOANS_DISBURSED ...\n",
            "      Page 17: -19- xxxiv) The Borrower agrees and undertakes tha...\n",
            "   other: 5 pages\n",
            "      Page 2: ...\n",
            "      Page 3: 220- (vii) The Borrower agrees and undertakes to f...\n",
            "      Page 5: LOANAGREEMENT THIS AGREEMENT made at New Delhi, th...\n",
            "\n",
            "🔢 STEP 2: Sorting within each group...\n",
            "   ✓ Numbered pages sorted by dash numbers\n",
            "   ✓ Articles sorted by article numbers\n",
            "   ✓ Schedules sorted (keeping what works!)\n",
            "   ✓ Other pages kept in original order\n",
            "\n",
            "🎯 STEP 3: Arranging groups in document order...\n",
            "   Adding 6 numbered_pages\n",
            "      Page 22 (sort: 4): -4- Further interest All interest which ...\n",
            "      Page 7 (sort: 16): -16- viii) The Borrower  shall at its ow...\n",
            "      Page 18 (sort: 17): -17- xix) The Borrower agrees and undert...\n",
            "      Page 19 (sort: 18): -18- xxviii) The Borrower shall agree an...\n",
            "      Page 17 (sort: 19): -19- xxxiv) The Borrower agrees and unde...\n",
            "      Page 15 (sort: 21): -21 - D) CONDITIONS APPLICABLE TO LOANS_...\n",
            "   Adding 2 articles\n",
            "      Page 1 (sort: 4): 7- ARTICLE - IV APPOINTMENT QE NOMNEE DI...\n",
            "      Page 6 (sort: 6): -9 ARTICLE VI EFFECTIVE DATE QE AGREEMEN...\n",
            "   Adding 12 schedules\n",
            "      Page 8 (sort: 3): 22- ARTICLE : ! DEFINTIONS GENERAL CONDI...\n",
            "      Page 9 (sort: 5): 5- In the event of the Borrower, failing...\n",
            "      Page 24 (sort: 5): -14- SCHEDULE V AMORTISATION SCHEDULE No...\n",
            "      Page 13 (sort: 6): -15- SCHEDULE VI SPECIAL CONDIIIQNS A) P...\n",
            "      Page 20 (sort: 6): -8- ARTICLE 3 v SPECIAL CONDITIONS The L...\n",
            "      Page 16 (sort: 10): -10- SCHEDULE ! Particulars of Loan Name...\n",
            "      Page 23 (sort: 11): -11- SCHEDULE ! The Project Installation...\n",
            "      Page 12 (sort: 12): -12- SCHEDULE W FIANCING PLAN A) Project...\n",
            "      Page 25 (sort: 13): -13- SCHEDULE W PARTICULARS QE WTERESI N...\n",
            "      Page 4 (sort: 999): LOANAGREEMENT No. Dated (Project No: Bor...\n",
            "      Page 10 (sort: 999): 6- ARTICLE - SECURITY 3.1 SECURITY FOR T...\n",
            "      Page 21 (sort: 999): 3- ARTICLE - ! THELQAN 2.1 AMQUNT AND TE...\n",
            "   Adding 5 other\n",
            "      Page 2 (sort: ?): ...\n",
            "      Page 3 (sort: ?): 220- (vii) The Borrower agrees and under...\n",
            "      Page 5 (sort: ?): LOANAGREEMENT THIS AGREEMENT made at New...\n",
            "      Page 11 (sort: ?): LOAN AGREEMENT DATED 20_ BETWEEN MIS LIM...\n",
            "      Page 14 (sort: ?): 222- IN WITNESS WHEREOF the Borrower has...\n",
            "\n",
            "📄 Creating final PDF...\n",
            "   Position 1 ← Original Page 22\n",
            "   Position 2 ← Original Page 7\n",
            "   Position 3 ← Original Page 18\n",
            "   Position 4 ← Original Page 19\n",
            "   Position 5 ← Original Page 17\n",
            "   Position 6 ← Original Page 15\n",
            "   Position 7 ← Original Page 1\n",
            "   Position 8 ← Original Page 6\n",
            "   Position 9 ← Original Page 8\n",
            "   Position 10 ← Original Page 9\n",
            "   Position 11 ← Original Page 24\n",
            "   Position 12 ← Original Page 13\n",
            "   Position 13 ← Original Page 20\n",
            "   Position 14 ← Original Page 16\n",
            "   Position 15 ← Original Page 23\n",
            "   Position 16 ← Original Page 12\n",
            "   Position 17 ← Original Page 25\n",
            "   Position 18 ← Original Page 4\n",
            "   Position 19 ← Original Page 10\n",
            "   Position 20 ← Original Page 21\n",
            "   Position 21 ← Original Page 2\n",
            "   Position 22 ← Original Page 3\n",
            "   Position 23 ← Original Page 5\n",
            "   Position 24 ← Original Page 11\n",
            "   Position 25 ← Original Page 14\n",
            "\n",
            "🎉 Step-by-step reordering complete: /content/reordered_step_by_step.pdf\n",
            "\n",
            "==================================================\n",
            "🧩 STEP-BY-STEP SUMMARY\n",
            "==================================================\n",
            "📁 Result: /content/reordered_step_by_step.pdf\n",
            "🔄 Final order: [22, 7, 18, 19, 17, 15, 1, 6, 8, 9, 24, 13, 20, 16, 23, 12, 25, 4, 10, 21, 2, 3, 5, 11, 14]\n",
            "\n",
            "🎯 What we did:\n",
            "   1. Grouped pages by type (schedules, articles, etc.)\n",
            "   2. Sorted within each group by their numbers\n",
            "   3. Arranged groups in logical document order\n",
            "\n",
            "📊 Group breakdown:\n",
            "   numbered_pages: 6 pages\n",
            "   articles: 2 pages\n",
            "   schedules: 12 pages\n",
            "   other: 5 pages\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pymupdf easyocr\n",
        "\n",
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "def extract_text_focused(pdf_path):\n",
        "    \"\"\"Extract text - simple and focused\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    page_data = []\n",
        "\n",
        "    print(f\"🔍 Analyzing {len(doc)} pages - step by step approach...\")\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        native_text = page.get_text()\n",
        "        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))\n",
        "        img_path = tempfile.mktemp(suffix='.png')\n",
        "        pix.save(img_path)\n",
        "\n",
        "        ocr_result = reader.readtext(img_path, detail=True)\n",
        "        img_height = pix.height\n",
        "        top_threshold = img_height * 0.2\n",
        "\n",
        "        top_text = \"\"\n",
        "        full_text = \"\"\n",
        "\n",
        "        for (bbox, text, conf) in ocr_result:\n",
        "            full_text += text + \" \"\n",
        "            y_coord = bbox[0][1]\n",
        "            if y_coord <= top_threshold:\n",
        "                top_text += text + \" \"\n",
        "\n",
        "        best_text = native_text if len(native_text) > 50 else full_text\n",
        "\n",
        "        page_data.append({\n",
        "            'page_num': page_num,\n",
        "            'top_text': top_text.strip(),\n",
        "            'full_text': best_text.strip(),\n",
        "        })\n",
        "\n",
        "        print(f\"📄 Page {page_num + 1}\")\n",
        "        os.remove(img_path)\n",
        "\n",
        "    doc.close()\n",
        "    return page_data\n",
        "\n",
        "def step1_group_by_type(page_data):\n",
        "    \"\"\"STEP 1: Just group pages by their obvious type\"\"\"\n",
        "    print(\"\\n📋 STEP 1: Grouping pages by type...\")\n",
        "\n",
        "    groups = {\n",
        "        'schedules': [],\n",
        "        'articles': [],\n",
        "        'numbered_pages': [],\n",
        "        'other': []\n",
        "    }\n",
        "\n",
        "    for data in page_data:\n",
        "        page_num = data['page_num']\n",
        "        full_text = data['full_text'].upper()\n",
        "\n",
        "        # Simple grouping logic\n",
        "        if 'SCHEDULE' in full_text:\n",
        "            groups['schedules'].append(data)\n",
        "        elif 'ARTICLE' in full_text:\n",
        "            groups['articles'].append(data)\n",
        "        elif re.search(r'-\\s*\\d+\\s*-', data['top_text']) or re.search(r'-\\s*\\d+\\s*-', data['full_text']):\n",
        "            groups['numbered_pages'].append(data)\n",
        "        else:\n",
        "            groups['other'].append(data)\n",
        "\n",
        "    # Show what we found\n",
        "    for group_name, pages in groups.items():\n",
        "        print(f\"   {group_name}: {len(pages)} pages\")\n",
        "        for page in pages[:3]:  # Show first 3\n",
        "            preview = page['full_text'][:50].replace('\\n', ' ')\n",
        "            print(f\"      Page {page['page_num']+1}: {preview}...\")\n",
        "\n",
        "    return groups\n",
        "\n",
        "def step2_sort_within_groups(groups):\n",
        "    \"\"\"STEP 2: Sort pages within each group\"\"\"\n",
        "    print(\"\\n🔢 STEP 2: Sorting within each group...\")\n",
        "\n",
        "    def extract_number(text, patterns):\n",
        "        \"\"\"Helper to extract numbers from text\"\"\"\n",
        "        for pattern in patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                match = matches[0]\n",
        "                if match.isdigit():\n",
        "                    return int(match)\n",
        "                # Simple roman numeral handling\n",
        "                roman_map = {'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5, 'VI': 6, 'VII': 7}\n",
        "                return roman_map.get(match.upper(), 999)\n",
        "        return 999\n",
        "\n",
        "    sorted_groups = {}\n",
        "\n",
        "    # Sort numbered pages by dash numbers\n",
        "    numbered = groups['numbered_pages']\n",
        "    for page in numbered:\n",
        "        dash_patterns = [r'-\\s*(\\d+)\\s*-']\n",
        "        page['sort_key'] = extract_number(page['top_text'] + ' ' + page['full_text'], dash_patterns)\n",
        "    numbered.sort(key=lambda x: x['sort_key'])\n",
        "    sorted_groups['numbered_pages'] = numbered\n",
        "    print(f\"   ✓ Numbered pages sorted by dash numbers\")\n",
        "\n",
        "    # Sort articles by article numbers\n",
        "    articles = groups['articles']\n",
        "    for page in articles:\n",
        "        article_patterns = [r'ARTICLE[:\\s]*([IVXLCDM\\d]+)', r'ARTICLE[:\\-\\s]*([IVXLCDM\\d]+)']\n",
        "        page['sort_key'] = extract_number(page['full_text'], article_patterns)\n",
        "    articles.sort(key=lambda x: x['sort_key'])\n",
        "    sorted_groups['articles'] = articles\n",
        "    print(f\"   ✓ Articles sorted by article numbers\")\n",
        "\n",
        "    # Sort schedules (they were already good!)\n",
        "    schedules = groups['schedules']\n",
        "    for page in schedules:\n",
        "        schedule_patterns = [r'SCHEDULE[:\\s]*([IVXLCDM\\d]+)', r'-\\s*(\\d+)\\s*-']\n",
        "        page['sort_key'] = extract_number(page['top_text'] + ' ' + page['full_text'], schedule_patterns)\n",
        "    schedules.sort(key=lambda x: x['sort_key'])\n",
        "    sorted_groups['schedules'] = schedules\n",
        "    print(f\"   ✓ Schedules sorted (keeping what works!)\")\n",
        "\n",
        "    # Other pages - just keep original order\n",
        "    sorted_groups['other'] = groups['other']\n",
        "    print(f\"   ✓ Other pages kept in original order\")\n",
        "\n",
        "    return sorted_groups\n",
        "\n",
        "def step3_arrange_groups(sorted_groups):\n",
        "    \"\"\"STEP 3: Arrange the groups in logical order\"\"\"\n",
        "    print(\"\\n🎯 STEP 3: Arranging groups in document order...\")\n",
        "\n",
        "    # Typical document flow: numbered pages → articles → schedules → other\n",
        "    group_order = ['numbered_pages', 'articles', 'schedules', 'other']\n",
        "\n",
        "    final_order = []\n",
        "\n",
        "    for group_name in group_order:\n",
        "        pages = sorted_groups.get(group_name, [])\n",
        "        if pages:\n",
        "            print(f\"   Adding {len(pages)} {group_name}\")\n",
        "            for page in pages:\n",
        "                final_order.append(page['page_num'])\n",
        "                sort_key = page.get('sort_key', '?')\n",
        "                preview = page['full_text'][:40].replace('\\n', ' ')\n",
        "                print(f\"      Page {page['page_num']+1} (sort: {sort_key}): {preview}...\")\n",
        "\n",
        "    return final_order\n",
        "\n",
        "def step_by_step_reorder(input_pdf_path, output_pdf_path):\n",
        "    \"\"\"Main function - step by step approach\"\"\"\n",
        "    print(\"🚀 Starting STEP-BY-STEP PDF reordering...\")\n",
        "\n",
        "    # Extract text\n",
        "    page_data = extract_text_focused(input_pdf_path)\n",
        "\n",
        "    # Step 1: Group by type\n",
        "    groups = step1_group_by_type(page_data)\n",
        "\n",
        "    # Step 2: Sort within groups\n",
        "    sorted_groups = step2_sort_within_groups(groups)\n",
        "\n",
        "    # Step 3: Arrange groups\n",
        "    final_order = step3_arrange_groups(sorted_groups)\n",
        "\n",
        "    # Create reordered PDF\n",
        "    doc = fitz.open(input_pdf_path)\n",
        "    reordered_doc = fitz.open()\n",
        "\n",
        "    print(\"\\n📄 Creating final PDF...\")\n",
        "    for new_pos, orig_idx in enumerate(final_order):\n",
        "        reordered_doc.insert_pdf(doc, from_page=orig_idx, to_page=orig_idx)\n",
        "        print(f\"   Position {new_pos + 1} ← Original Page {orig_idx + 1}\")\n",
        "\n",
        "    reordered_doc.save(output_pdf_path)\n",
        "    doc.close()\n",
        "    reordered_doc.close()\n",
        "\n",
        "    print(f\"\\n🎉 Step-by-step reordering complete: {output_pdf_path}\")\n",
        "\n",
        "    return final_order, sorted_groups\n",
        "\n",
        "# USAGE - Step by step approach\n",
        "input_pdf = '/content/jumbled.pdf'\n",
        "output_pdf = '/content/reordered_step_by_step.pdf'\n",
        "\n",
        "try:\n",
        "    final_order, groups = step_by_step_reorder(input_pdf, output_pdf)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"🧩 STEP-BY-STEP SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"📁 Result: {output_pdf}\")\n",
        "    print(f\"🔄 Final order: {[x+1 for x in final_order]}\")\n",
        "\n",
        "    print(\"\\n🎯 What we did:\")\n",
        "    print(\"   1. Grouped pages by type (schedules, articles, etc.)\")\n",
        "    print(\"   2. Sorted within each group by their numbers\")\n",
        "    print(\"   3. Arranged groups in logical document order\")\n",
        "\n",
        "    print(\"\\n📊 Group breakdown:\")\n",
        "    for group_name, pages in groups.items():\n",
        "        print(f\"   {group_name}: {len(pages)} pages\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExEUVOx6QMBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced detector for tricky page numbers\n",
        "\n",
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "\n",
        "# Initialize EasyOCR\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "def enhanced_page_detection(pdf_path):\n",
        "    \"\"\"Enhanced detection with multiple OCR strategies\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    all_pages_detailed = []\n",
        "\n",
        "    print(f\"🔍 ENHANCED DETECTION on {len(doc)} pages...\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "\n",
        "        print(f\"\\n📄 ANALYZING PDF Position {page_num + 1}:\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Strategy 1: Native PDF text extraction\n",
        "        native_text = page.get_text()\n",
        "        native_pages = find_page_numbers_comprehensive(native_text, \"NATIVE\")\n",
        "\n",
        "        # Strategy 2: Standard OCR\n",
        "        standard_pages = []\n",
        "        try:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))\n",
        "            img_path = tempfile.mktemp(suffix='.png')\n",
        "            pix.save(img_path)\n",
        "\n",
        "            ocr_results = reader.readtext(img_path, detail=True)\n",
        "            ocr_text = ' '.join([text for _, text, conf in ocr_results if conf > 0.3])\n",
        "            standard_pages = find_page_numbers_comprehensive(ocr_text, \"STANDARD_OCR\")\n",
        "\n",
        "            os.remove(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Standard OCR failed: {e}\")\n",
        "\n",
        "        # Strategy 3: Enhanced OCR (higher resolution + preprocessing)\n",
        "        enhanced_pages = []\n",
        "        try:\n",
        "            # Higher resolution\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(6, 6))  # 6x instead of 3x\n",
        "            img_path = tempfile.mktemp(suffix='.png')\n",
        "            pix.save(img_path)\n",
        "\n",
        "            # Preprocess image for better OCR\n",
        "            img = Image.open(img_path)\n",
        "\n",
        "            # Enhance contrast\n",
        "            enhancer = ImageEnhance.Contrast(img)\n",
        "            img = enhancer.enhance(2.0)\n",
        "\n",
        "            # Convert to grayscale and apply sharpening\n",
        "            img = img.convert('L')\n",
        "            img = img.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "            # Save preprocessed image\n",
        "            enhanced_path = tempfile.mktemp(suffix='.png')\n",
        "            img.save(enhanced_path)\n",
        "\n",
        "            # OCR with different settings\n",
        "            ocr_results = reader.readtext(enhanced_path, detail=True, width_ths=0.3, height_ths=0.3)\n",
        "            enhanced_text = ' '.join([text for _, text, conf in ocr_results if conf > 0.2])\n",
        "            enhanced_pages = find_page_numbers_comprehensive(enhanced_text, \"ENHANCED_OCR\")\n",
        "\n",
        "            os.remove(img_path)\n",
        "            os.remove(enhanced_path)\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Enhanced OCR failed: {e}\")\n",
        "\n",
        "        # Strategy 4: Region-specific OCR (focus on header areas)\n",
        "        region_pages = []\n",
        "        try:\n",
        "            # Focus on top 25% of page\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(4, 4))\n",
        "            img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, 3)\n",
        "\n",
        "            # Extract header region\n",
        "            header_height = int(pix.height * 0.25)\n",
        "            header_region = img_array[:header_height, :, :]\n",
        "\n",
        "            # Save header region\n",
        "            header_img = Image.fromarray(header_region)\n",
        "            header_path = tempfile.mktemp(suffix='.png')\n",
        "            header_img.save(header_path)\n",
        "\n",
        "            # OCR on header only\n",
        "            header_results = reader.readtext(header_path, detail=True)\n",
        "            header_text = ' '.join([text for _, text, conf in header_results if conf > 0.2])\n",
        "            region_pages = find_page_numbers_comprehensive(header_text, \"HEADER_REGION\")\n",
        "\n",
        "            os.remove(header_path)\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Region OCR failed: {e}\")\n",
        "\n",
        "        # Strategy 5: Raw character detection (for very simple cases)\n",
        "        raw_pages = []\n",
        "        try:\n",
        "            # Look for simple patterns in raw text\n",
        "            all_raw_text = native_text + \" \" + ocr_text + \" \" + enhanced_text + \" \" + header_text\n",
        "            raw_pages = find_simple_dash_patterns(all_raw_text)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Combine all findings\n",
        "        all_found_pages = native_pages + standard_pages + enhanced_pages + region_pages + raw_pages\n",
        "\n",
        "        # Remove duplicates and sort by confidence\n",
        "        unique_pages = {}\n",
        "        for page_info in all_found_pages:\n",
        "            page_num_found = page_info['value']\n",
        "            if page_num_found not in unique_pages or page_info['confidence'] > unique_pages[page_num_found]['confidence']:\n",
        "                unique_pages[page_num_found] = page_info\n",
        "\n",
        "        final_pages = list(unique_pages.values())\n",
        "        final_pages.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "        # Show results\n",
        "        if final_pages:\n",
        "            print(f\"   ✅ FOUND PAGE NUMBERS: {[p['value'] for p in final_pages]}\")\n",
        "            for p in final_pages:\n",
        "                print(f\"      -{p['value']}- (Confidence: {p['confidence']:.1f}, Method: {p['method']})\")\n",
        "        else:\n",
        "            print(f\"   ❌ NO PAGE NUMBERS DETECTED\")\n",
        "\n",
        "        # Preview text\n",
        "        preview_text = (native_text + \" \" + ocr_text)[:200].replace('\\n', ' ').strip()\n",
        "        print(f\"   📝 Text Preview: {preview_text}...\")\n",
        "\n",
        "        page_info = {\n",
        "            'pdf_position': page_num,\n",
        "            'found_pages': final_pages,\n",
        "            'best_page': final_pages[0] if final_pages else None,\n",
        "            'text_preview': preview_text\n",
        "        }\n",
        "\n",
        "        all_pages_detailed.append(page_info)\n",
        "\n",
        "    doc.close()\n",
        "    return all_pages_detailed\n",
        "\n",
        "def find_page_numbers_comprehensive(text, method_name):\n",
        "    \"\"\"Find page numbers with multiple pattern variations\"\"\"\n",
        "    found = []\n",
        "\n",
        "    if not text:\n",
        "        return found\n",
        "\n",
        "    # Pattern 1: Standard dash format with various spacing\n",
        "    patterns = [\n",
        "        r'-\\s*(\\d+)\\s*-',           # -2- or - 2 - or -2 -\n",
        "        r'—\\s*(\\d+)\\s*—',           # em-dash version\n",
        "        r'–\\s*(\\d+)\\s*–',           # en-dash version\n",
        "        r'\\-\\s*(\\d+)\\s*\\-',         # escaped dashes\n",
        "        r'[\\-—–]\\s*(\\d+)\\s*[\\-—–]', # any dash type\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                page_num = int(match)\n",
        "                if 1 <= page_num <= 100:  # Reasonable range\n",
        "                    found.append({\n",
        "                        'value': page_num,\n",
        "                        'confidence': 90 if method_name == \"NATIVE\" else 80,\n",
        "                        'method': method_name,\n",
        "                        'pattern': f'-{match}-'\n",
        "                    })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Pattern 2: Page at start/end of line\n",
        "    line_patterns = [\n",
        "        r'^\\s*-\\s*(\\d+)\\s*-\\s*$',   # -2- on its own line\n",
        "        r'^\\s*(\\d+)\\s*$',           # Just number on its own line\n",
        "    ]\n",
        "\n",
        "    for pattern in line_patterns:\n",
        "        matches = re.findall(pattern, text, re.MULTILINE)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                page_num = int(match)\n",
        "                if 1 <= page_num <= 100:\n",
        "                    found.append({\n",
        "                        'value': page_num,\n",
        "                        'confidence': 85 if method_name == \"NATIVE\" else 75,\n",
        "                        'method': f\"{method_name}_LINE\",\n",
        "                        'pattern': f'line:{match}'\n",
        "                    })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return found\n",
        "\n",
        "def find_simple_dash_patterns(text):\n",
        "    \"\"\"Simple pattern matching for missed cases\"\"\"\n",
        "    found = []\n",
        "\n",
        "    # Look for any sequence that might be a page number\n",
        "    # This catches cases where OCR might have mangled the dashes\n",
        "    patterns = [\n",
        "        r'[^\\w](\\d+)[^\\w]',  # Number surrounded by non-word chars\n",
        "        r'\\b(\\d+)\\b',        # Word boundary numbers\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        for match in matches:\n",
        "            try:\n",
        "                page_num = int(match)\n",
        "                if 1 <= page_num <= 50 and len(match) <= 2:  # Single or double digit\n",
        "                    found.append({\n",
        "                        'value': page_num,\n",
        "                        'confidence': 60,  # Lower confidence\n",
        "                        'method': \"RAW_PATTERN\",\n",
        "                        'pattern': match\n",
        "                    })\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    return found\n",
        "\n",
        "def create_final_order(detailed_pages):\n",
        "    \"\"\"Create final ordering from detailed analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"📋 FINAL ORDERING ANALYSIS:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Extract pages with their positions\n",
        "    page_mapping = {}\n",
        "\n",
        "    for page_info in detailed_pages:\n",
        "        pdf_pos = page_info['pdf_position']\n",
        "        if page_info['best_page']:\n",
        "            page_num = page_info['best_page']['value']\n",
        "            confidence = page_info['best_page']['confidence']\n",
        "            method = page_info['best_page']['method']\n",
        "\n",
        "            page_mapping[page_num] = {\n",
        "                'pdf_position': pdf_pos,\n",
        "                'confidence': confidence,\n",
        "                'method': method\n",
        "            }\n",
        "\n",
        "    if not page_mapping:\n",
        "        print(\"❌ No page numbers found!\")\n",
        "        return None\n",
        "\n",
        "    # Sort by page number\n",
        "    sorted_pages = sorted(page_mapping.keys())\n",
        "    final_order = []\n",
        "\n",
        "    print(f\"Found pages: {sorted_pages}\")\n",
        "    print()\n",
        "\n",
        "    for i, page_num in enumerate(sorted_pages):\n",
        "        info = page_mapping[page_num]\n",
        "        pdf_pos = info['pdf_position']\n",
        "        confidence = info['confidence']\n",
        "        method = info['method']\n",
        "\n",
        "        final_order.append(pdf_pos)\n",
        "\n",
        "        print(f\"Position {i+1}: Page -{page_num}- (PDF position {pdf_pos + 1})\")\n",
        "        print(f\"   Confidence: {confidence:.1f} | Method: {method}\")\n",
        "\n",
        "    return final_order\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def run_enhanced_detection(pdf_path):\n",
        "    \"\"\"Run enhanced detection to catch all page numbers\"\"\"\n",
        "    print(\"🚀 STARTING ENHANCED PAGE NUMBER DETECTION...\")\n",
        "\n",
        "    detailed_results = enhanced_page_detection(pdf_path)\n",
        "    final_order = create_final_order(detailed_results)\n",
        "\n",
        "    if final_order:\n",
        "        print(f\"\\n✅ FINAL REORDERING SEQUENCE:\")\n",
        "        print(f\"PDF positions: {[x + 1 for x in final_order]}\")\n",
        "        return final_order, detailed_results\n",
        "    else:\n",
        "        print(f\"\\n❌ Could not determine page order\")\n",
        "        return None, detailed_results\n",
        "\n",
        "# USAGE\n",
        "pdf_path = '/content/jumbled.pdf'\n",
        "\n",
        "try:\n",
        "    order, details = run_enhanced_detection(pdf_path)\n",
        "    if order:\n",
        "        print(f\"\\n🎉 SUCCESS! Found {len(order)} ordered pages\")\n",
        "        print(f\"Order: {[x+1 for x in order]}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "id": "FqG2z8zYNj-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete 25-Page PDF Analyzer - Shows Every Single Page\n",
        "\n",
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize EasyOCR\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "class Complete25PageAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.all_pages_data = []\n",
        "        self.confidence_threshold = 75  # Lower threshold to catch more pages\n",
        "\n",
        "    def analyze_every_single_page(self, pdf_path):\n",
        "        \"\"\"Analyze every single page with detailed breakdown\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        self.all_pages_data = []\n",
        "\n",
        "        print(f\"🔍 ANALYZING ALL {len(doc)} PAGES IN DETAIL...\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "\n",
        "            print(f\"\\n{'='*20} PDF POSITION {page_num + 1} {'='*20}\")\n",
        "\n",
        "            # Get all text sources\n",
        "            native_text = page.get_text()\n",
        "\n",
        "            # Enhanced OCR\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(4, 4))\n",
        "                img_path = tempfile.mktemp(suffix='.png')\n",
        "                pix.save(img_path)\n",
        "\n",
        "                # Preprocess image\n",
        "                img = Image.open(img_path)\n",
        "                enhancer = ImageEnhance.Contrast(img)\n",
        "                img = enhancer.enhance(1.5)\n",
        "                img = img.convert('L')\n",
        "\n",
        "                enhanced_path = tempfile.mktemp(suffix='.png')\n",
        "                img.save(enhanced_path)\n",
        "\n",
        "                ocr_results = reader.readtext(enhanced_path, detail=True)\n",
        "                ocr_text = ' '.join([text for _, text, conf in ocr_results if conf > 0.2])\n",
        "\n",
        "                os.remove(img_path)\n",
        "                os.remove(enhanced_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                ocr_text = \"\"\n",
        "                print(f\"OCR failed: {e}\")\n",
        "\n",
        "            # Combine all text\n",
        "            full_text = native_text + \" \" + ocr_text\n",
        "\n",
        "            # Find ALL possible page indicators (very permissive)\n",
        "            page_indicators = self._find_all_possible_indicators(full_text, page_num + 1)\n",
        "\n",
        "            # Get meaningful text preview\n",
        "            meaningful_text = self._extract_meaningful_text(full_text)\n",
        "\n",
        "            # Store comprehensive data\n",
        "            page_data = {\n",
        "                'pdf_position': page_num,\n",
        "                'page_indicators': page_indicators,\n",
        "                'meaningful_text': meaningful_text,\n",
        "                'full_text_length': len(full_text.strip()),\n",
        "                'has_substantial_content': len(meaningful_text) > 50,\n",
        "                'text_preview': meaningful_text[:150] + \"...\" if len(meaningful_text) > 150 else meaningful_text\n",
        "            }\n",
        "\n",
        "            self.all_pages_data.append(page_data)\n",
        "\n",
        "            # Print detailed breakdown\n",
        "            print(f\"📄 Content Length: {len(full_text.strip())} characters\")\n",
        "            print(f\"📊 Page Indicators Found: {len(page_indicators)}\")\n",
        "\n",
        "            if page_indicators:\n",
        "                for indicator in page_indicators[:3]:  # Show top 3\n",
        "                    print(f\"   🎯 -{indicator['value']}- (Confidence: {indicator['confidence']:.1f}, Method: {indicator['method']})\")\n",
        "\n",
        "            print(f\"📝 Content Preview:\")\n",
        "            print(f\"   {meaningful_text[:200]}...\")\n",
        "\n",
        "            # Special analysis for specific patterns\n",
        "            if self._looks_like_cover_page(meaningful_text):\n",
        "                print(\"   🏷️  LIKELY: Cover Page\")\n",
        "            elif self._looks_like_toc(meaningful_text):\n",
        "                print(\"   🏷️  LIKELY: Table of Contents\")\n",
        "            elif self._looks_like_signature_page(meaningful_text):\n",
        "                print(\"   🏷️  LIKELY: Signature Page\")\n",
        "            elif len(meaningful_text.strip()) < 20:\n",
        "                print(\"   🏷️  LIKELY: Blank/Nearly Blank Page\")\n",
        "\n",
        "        doc.close()\n",
        "        return self.all_pages_data\n",
        "\n",
        "    def _find_all_possible_indicators(self, text, pdf_pos):\n",
        "        \"\"\"Find all possible page indicators with very permissive matching\"\"\"\n",
        "        indicators = []\n",
        "\n",
        "        if not text:\n",
        "            return indicators\n",
        "\n",
        "        # Primary patterns (high confidence)\n",
        "        primary_patterns = [\n",
        "            (r'-\\s*(\\d+)\\s*-', 90, \"DASH_FORMAT\"),\n",
        "            (r'—\\s*(\\d+)\\s*—', 90, \"EM_DASH\"),\n",
        "            (r'–\\s*(\\d+)\\s*–', 90, \"EN_DASH\"),\n",
        "            (r'^\\s*-\\s*(\\d+)\\s*-\\s*$', 95, \"ISOLATED_DASH\"),\n",
        "        ]\n",
        "\n",
        "        # Secondary patterns (medium confidence)\n",
        "        secondary_patterns = [\n",
        "            (r'page\\s+(\\d+)', 75, \"PAGE_WORD\"),\n",
        "            (r'p\\.?\\s*(\\d+)', 70, \"P_DOT\"),\n",
        "            (r'(\\d+)\\s*of\\s*\\d+', 80, \"X_OF_Y\"),\n",
        "            (r'article\\s*:?\\s*([IVXLCDM]+|\\d+)', 70, \"ARTICLE\"),\n",
        "        ]\n",
        "\n",
        "        # Tertiary patterns (lower confidence, catches edge cases)\n",
        "        tertiary_patterns = [\n",
        "            (r'\\b(\\d+)\\b', 50, \"STANDALONE_NUMBER\"),\n",
        "            (r'^(\\d+)$', 60, \"LINE_NUMBER\"),\n",
        "            (r'^\\s*(\\d+)\\s*$', 55, \"ISOLATED_NUMBER\"),\n",
        "        ]\n",
        "\n",
        "        all_patterns = primary_patterns + secondary_patterns + tertiary_patterns\n",
        "\n",
        "        for pattern, base_conf, method in all_patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    if method == \"ARTICLE\" and not match.isdigit():\n",
        "                        # Convert Roman numerals\n",
        "                        value = self._roman_to_decimal(match.upper())\n",
        "                    else:\n",
        "                        value = int(match)\n",
        "\n",
        "                    # Reasonable range and avoid obvious false positives\n",
        "                    if 1 <= value <= 100:\n",
        "                        # Bonus confidence if number matches expected range\n",
        "                        if 1 <= value <= 30:  # Typical document page range\n",
        "                            confidence = base_conf * 1.1\n",
        "                        else:\n",
        "                            confidence = base_conf * 0.8\n",
        "\n",
        "                        # Avoid obvious date years as page numbers\n",
        "                        if value > 1900 and method == \"STANDALONE_NUMBER\":\n",
        "                            confidence *= 0.3\n",
        "\n",
        "                        indicators.append({\n",
        "                            'value': value,\n",
        "                            'confidence': min(confidence, 100),\n",
        "                            'method': method,\n",
        "                            'pattern_match': match,\n",
        "                            'found_in_pdf_pos': pdf_pos\n",
        "                        })\n",
        "\n",
        "                except (ValueError, AttributeError):\n",
        "                    continue\n",
        "\n",
        "        # Sort by confidence, remove duplicates\n",
        "        seen_values = {}\n",
        "        for indicator in indicators:\n",
        "            value = indicator['value']\n",
        "            if value not in seen_values or indicator['confidence'] > seen_values[value]['confidence']:\n",
        "                seen_values[value] = indicator\n",
        "\n",
        "        return list(seen_values.values())\n",
        "\n",
        "    def _roman_to_decimal(self, roman):\n",
        "        \"\"\"Convert Roman numerals to decimal\"\"\"\n",
        "        values = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100}\n",
        "        total = 0\n",
        "        prev_value = 0\n",
        "        for char in reversed(roman):\n",
        "            value = values.get(char, 0)\n",
        "            if value < prev_value:\n",
        "                total -= value\n",
        "            else:\n",
        "                total += value\n",
        "            prev_value = value\n",
        "        return total\n",
        "\n",
        "    def _extract_meaningful_text(self, text):\n",
        "        \"\"\"Extract meaningful text, removing excessive whitespace and noise\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        cleaned = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove common OCR artifacts\n",
        "        cleaned = re.sub(r'[|{}]+', '', cleaned)\n",
        "\n",
        "        # Focus on actual content\n",
        "        lines = [line.strip() for line in cleaned.split('\\n') if len(line.strip()) > 3]\n",
        "\n",
        "        return ' '.join(lines)\n",
        "\n",
        "    def _looks_like_cover_page(self, text):\n",
        "        \"\"\"Detect if this looks like a cover page\"\"\"\n",
        "        cover_indicators = ['loan agreement', 'agreement', 'between', 'borrower', 'lender', 'dated']\n",
        "        text_lower = text.lower()\n",
        "        return sum(1 for indicator in cover_indicators if indicator in text_lower) >= 3\n",
        "\n",
        "    def _looks_like_toc(self, text):\n",
        "        \"\"\"Detect table of contents\"\"\"\n",
        "        toc_indicators = ['contents', 'article', 'section', 'schedule', 'page']\n",
        "        text_lower = text.lower()\n",
        "        return sum(1 for indicator in toc_indicators if indicator in text_lower) >= 2\n",
        "\n",
        "    def _looks_like_signature_page(self, text):\n",
        "        \"\"\"Detect signature page\"\"\"\n",
        "        sig_indicators = ['witness', 'signature', 'seal', 'signed', 'executed']\n",
        "        text_lower = text.lower()\n",
        "        return sum(1 for indicator in sig_indicators if indicator in text_lower) >= 2\n",
        "\n",
        "    def create_comprehensive_order(self):\n",
        "        \"\"\"Create comprehensive page ordering using all available data\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "        print(\"📋 CREATING COMPREHENSIVE PAGE ORDER FROM ALL 25 PAGES\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        # Collect all page numbers with their best positions\n",
        "        page_position_map = {}\n",
        "        confidence_map = {}\n",
        "\n",
        "        for page_data in self.all_pages_data:\n",
        "            pdf_pos = page_data['pdf_position']\n",
        "\n",
        "            for indicator in page_data['page_indicators']:\n",
        "                page_num = indicator['value']\n",
        "                confidence = indicator['confidence']\n",
        "\n",
        "                # Only accept if confidence is above threshold OR if it's the best we have for this page number\n",
        "                if (confidence >= self.confidence_threshold or\n",
        "                    page_num not in confidence_map or\n",
        "                    confidence > confidence_map[page_num]):\n",
        "\n",
        "                    page_position_map[page_num] = pdf_pos\n",
        "                    confidence_map[page_num] = confidence\n",
        "\n",
        "        # Create ordered sequence\n",
        "        if not page_position_map:\n",
        "            print(\"❌ No page numbers found with sufficient confidence!\")\n",
        "            return None, None\n",
        "\n",
        "        sorted_page_numbers = sorted(page_position_map.keys())\n",
        "        ordered_positions = [page_position_map[page_num] for page_num in sorted_page_numbers]\n",
        "\n",
        "        print(f\"📊 COMPREHENSIVE RESULTS:\")\n",
        "        print(f\"   Total PDF pages: 25\")\n",
        "        print(f\"   Pages with numbers found: {len(sorted_page_numbers)}\")\n",
        "        print(f\"   Page numbers found: {sorted_page_numbers}\")\n",
        "        print(f\"   Confidence threshold used: {self.confidence_threshold}\")\n",
        "        print()\n",
        "\n",
        "        print(f\"📋 FINAL ORDERING:\")\n",
        "        for i, page_num in enumerate(sorted_page_numbers):\n",
        "            pdf_pos = page_position_map[page_num]\n",
        "            confidence = confidence_map[page_num]\n",
        "            print(f\"   Position {i+1}: Page -{page_num}- (PDF position {pdf_pos + 1}) [Confidence: {confidence:.1f}]\")\n",
        "\n",
        "        # Show unordered pages\n",
        "        ordered_pdf_positions = set(ordered_positions)\n",
        "        unordered_positions = [i for i in range(25) if i not in ordered_pdf_positions]\n",
        "\n",
        "        if unordered_positions:\n",
        "            print(f\"\\n📄 PAGES WITHOUT CLEAR NUMBERING:\")\n",
        "            for pdf_pos in unordered_positions:\n",
        "                page_data = self.all_pages_data[pdf_pos]\n",
        "                preview = page_data['text_preview'][:100]\n",
        "                print(f\"   PDF position {pdf_pos + 1}: {preview}...\")\n",
        "\n",
        "        return ordered_positions, {\n",
        "            'total_pages': 25,\n",
        "            'numbered_pages': len(sorted_page_numbers),\n",
        "            'unnumbered_pages': len(unordered_positions),\n",
        "            'page_numbers_found': sorted_page_numbers,\n",
        "            'confidence_threshold': self.confidence_threshold\n",
        "        }\n",
        "\n",
        "    def create_reordered_pdf(self, input_pdf_path, output_pdf_path, ordered_positions):\n",
        "        \"\"\"Create reordered PDF\"\"\"\n",
        "        if not ordered_positions:\n",
        "            print(\"❌ Cannot reorder - no page order available\")\n",
        "            return False\n",
        "\n",
        "        print(f\"\\n📄 Creating reordered PDF with {len(ordered_positions)} pages...\")\n",
        "\n",
        "        doc = fitz.open(input_pdf_path)\n",
        "        reordered_doc = fitz.open()\n",
        "\n",
        "        for new_pos, orig_pdf_pos in enumerate(ordered_positions):\n",
        "            reordered_doc.insert_pdf(doc, from_page=orig_pdf_pos, to_page=orig_pdf_pos)\n",
        "            print(f\"   Position {new_pos + 1} ← PDF page {orig_pdf_pos + 1}\")\n",
        "\n",
        "        reordered_doc.save(output_pdf_path)\n",
        "        doc.close()\n",
        "        reordered_doc.close()\n",
        "\n",
        "        print(f\"✅ Reordered PDF saved: {output_pdf_path}\")\n",
        "        return True\n",
        "\n",
        "    def process_complete_25_pages(self, input_pdf_path):\n",
        "        \"\"\"Complete processing of all 25 pages\"\"\"\n",
        "        print(\"🚀 PROCESSING ALL 25 PAGES COMPREHENSIVELY...\")\n",
        "\n",
        "        base_name = input_pdf_path.replace('.pdf', '')\n",
        "        output_pdf = f\"{base_name}_complete_reorder.pdf\"\n",
        "        report_path = f\"{base_name}_25_page_analysis.json\"\n",
        "\n",
        "        try:\n",
        "            # Analyze every page\n",
        "            self.analyze_every_single_page(input_pdf_path)\n",
        "\n",
        "            # Create comprehensive order\n",
        "            ordered_positions, summary = self.create_comprehensive_order()\n",
        "\n",
        "            if ordered_positions:\n",
        "                # Create reordered PDF\n",
        "                success = self.create_reordered_pdf(input_pdf_path, output_pdf, ordered_positions)\n",
        "\n",
        "                # Save detailed report\n",
        "                report_data = {\n",
        "                    'summary': summary,\n",
        "                    'all_pages_data': self.all_pages_data\n",
        "                }\n",
        "\n",
        "                with open(report_path, 'w') as f:\n",
        "                    json.dump(report_data, f, indent=2)\n",
        "\n",
        "                return {\n",
        "                    'success': success,\n",
        "                    'reordered_pdf': output_pdf,\n",
        "                    'analysis_report': report_path,\n",
        "                    'summary': summary\n",
        "                }\n",
        "            else:\n",
        "                return {'success': False, 'error': 'Could not determine page order'}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {'success': False, 'error': str(e)}\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def main():\n",
        "    \"\"\"Process the complete 25-page PDF\"\"\"\n",
        "    input_pdf = '/content/jumbled.pdf'  # Your PDF path\n",
        "\n",
        "    analyzer = Complete25PageAnalyzer()\n",
        "    result = analyzer.process_complete_25_pages(input_pdf)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"\\n🎉 SUCCESS - ALL 25 PAGES ANALYZED!\")\n",
        "        print(f\"📁 Reordered PDF: {result['reordered_pdf']}\")\n",
        "        print(f\"📊 Analysis Report: {result['analysis_report']}\")\n",
        "\n",
        "        summary = result['summary']\n",
        "        print(f\"\\n📋 FINAL SUMMARY:\")\n",
        "        print(f\"   Total pages processed: {summary['total_pages']}\")\n",
        "        print(f\"   Pages with numbers: {summary['numbered_pages']}\")\n",
        "        print(f\"   Pages without numbers: {summary['unnumbered_pages']}\")\n",
        "        print(f\"   Page numbers found: {summary['page_numbers_found']}\")\n",
        "    else:\n",
        "        print(f\"❌ Failed: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RSdbCt4Szlr",
        "outputId": "3c1e356e-bd45-4dea-b70d-dc75d935bfaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 PROCESSING ALL 25 PAGES COMPREHENSIVELY...\n",
            "🔍 ANALYZING ALL 25 PAGES IN DETAIL...\n",
            "====================================================================================================\n",
            "\n",
            "==================== PDF POSITION 1 ====================\n",
            "📄 Content Length: 246 characters\n",
            "📊 Page Indicators Found: 0\n",
            "📝 Content Preview:\n",
            "   ARTICLE _ WV APPOINTMENT QF NOMNEE DIRECTORS The Borrower agrees that IREDA shall be entitled to appoint and withdraw from time to time nominee directors on the Board of Directors of the Borrower at a...\n",
            "\n",
            "==================== PDF POSITION 2 ====================\n",
            "📄 Content Length: 0 characters\n",
            "📊 Page Indicators Found: 0\n",
            "📝 Content Preview:\n",
            "   ...\n",
            "   🏷️  LIKELY: Blank/Nearly Blank Page\n",
            "\n",
            "==================== PDF POSITION 3 ====================\n",
            "📄 Content Length: 1140 characters\n",
            "📊 Page Indicators Found: 0\n",
            "📝 Content Preview:\n",
            "   220- (vii) The Borrower agrees and undertakes to furnish certificate from the Chartered Company Secretary Company Secretary in the employment of the Borrower that no Director of the Company has been d...\n",
            "\n",
            "==================== PDF POSITION 4 ====================\n",
            "📄 Content Length: 338 characters\n",
            "📊 Page Indicators Found: 0\n",
            "📝 Content Preview:\n",
            "   LOAN AGREEMENT No. Dated (Project No. Borrower Mls_ Limited Details of Project Installation of (Project No. Loan Amount Rs. Lakhs Security Irrevocable Bank Guarantee from Scheduled Commercial Bank DIA...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 5 ====================\n",
            "📄 Content Length: 708 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -3- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   LOAN AGREEMENI THIS AGREEMENT made at New Delhi, this of 20_ between Mls Limited, a Company within the meaning of the Companies Act; 1956 of 1956) and having its Registered Office at in the State of (...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 6 ====================\n",
            "📄 Content Length: 630 characters\n",
            "📊 Page Indicators Found: 2\n",
            "   🎯 -5- (Confidence: 77.0, Method: ARTICLE)\n",
            "   🎯 -9- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -9 ARTICLE V EFFECTIVE DATE QE AGREEMENTL PLACE QE DISBURSEMENI QE LOAN BX RREDA AND REPAYMENT BY BORROWER ETC This Agreement shall become binding on the Borrower and IREDA on and from the date first ...\n",
            "\n",
            "==================== PDF POSITION 7 ====================\n",
            "📄 Content Length: 2668 characters\n",
            "📊 Page Indicators Found: 2\n",
            "   🎯 -16- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -25- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -16- viii) The Borrower shall at its own cost the equipments in good condition and keep the same insured: ix) The Borrower shall obtain the project clearance for the Project from State Electricity Boa...\n",
            "\n",
            "==================== PDF POSITION 8 ====================\n",
            "📄 Content Length: 833 characters\n",
            "📊 Page Indicators Found: 2\n",
            "   🎯 -2- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -1- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   ~2- ARTICLE : ! DEFINITIONS GENERAL CONDITIONS 1.1 DEFINITIONS For the purpose of this Agreement the following terms shall have the following meanings:- a) \"Financing Plan\" means the financing plan as...\n",
            "   🏷️  LIKELY: Table of Contents\n",
            "\n",
            "==================== PDF POSITION 9 ====================\n",
            "📄 Content Length: 1647 characters\n",
            "📊 Page Indicators Found: 6\n",
            "   🎯 -5- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -2- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -6- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   5- In the event of the Borrower, failing to pay the monies referred to in sub-clause (0) and (ii), IREDA will be at liberty (but shall not be obliged) to pay the same. The Borrower shall reimburse all...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 10 ====================\n",
            "📄 Content Length: 942 characters\n",
            "📊 Page Indicators Found: 3\n",
            "   🎯 -6- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -3- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -1- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   ~6- ARTICLE : SECURITY 3.1 SECURITY FOR THE LOAN The loan together with all interest, liquidated damages, commitment charges, premia on prepayments or on redemption, costs, expenses and other monies w...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 11 ====================\n",
            "📄 Content Length: 174 characters\n",
            "📊 Page Indicators Found: 0\n",
            "📝 Content Preview:\n",
            "   LOAN AGREEMENT DATED 20_ BETWEEN MIS LIMITED AS BORROWER AND INDIAN RENEWABLE ENERGY DEVELOPMENT AGENCY LIMITED (IREDA) AS LENDER (Rupee Loan) Secured against Bank Guarantee)...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 12 ====================\n",
            "📄 Content Length: 160 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -12- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -12- SCHEDULE W FINANCING PLAN A) Project Cost: S. No_ Description Cost (Rs. in lakhs B) Means_of_Finance_for Project Cost: SI: No_ Source Amount (Rs. in lakhs\"...\n",
            "\n",
            "==================== PDF POSITION 13 ====================\n",
            "📄 Content Length: 2398 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -15- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -15- SCHEDULE VI SPECIAL CONDITIONS A) Pre-disbursement /Loan Agreement Execution conditions Before availing of loan assistance from IREDA, the Borrower shall complyl agree to comply with the followin...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 14 ====================\n",
            "📄 Content Length: 707 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -22- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -22- IN WITNESS WHEREOF the Borrower has caused its Common Seal to be affixed hereto and to a duplicate hereof on the month and year first hereinabove written and IREDA have caused the same and the sa...\n",
            "   🏷️  LIKELY: Signature Page\n",
            "\n",
            "==================== PDF POSITION 15 ====================\n",
            "📄 Content Length: 1481 characters\n",
            "📊 Page Indicators Found: 3\n",
            "   🎯 -21- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -2- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -3- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -21- D) CONDITIONS APPLICABLE TO LOANS DISBURSED QUT OF WORLD BANK LINE OF CREDIT: The Borrower shall agree and undertake that the loan sanctioned for the project is out of the funds to be received by...\n",
            "\n",
            "==================== PDF POSITION 16 ====================\n",
            "📄 Content Length: 146 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -10- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -10- SCHEDULE ! Particulars of Loan Name of the Lender Total Loan Indian Renewable Energy Rs. lakhs (Rupees Development Agency Ltd. only): (IREDA)...\n",
            "\n",
            "==================== PDF POSITION 17 ====================\n",
            "📄 Content Length: 3286 characters\n",
            "📊 Page Indicators Found: 6\n",
            "   🎯 -19- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -10- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -50- (Confidence: 40.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -19- xxxiv) The Borrower agrees and undertakes that : the Borrower agrees and undertakes to furnish certificate from the Chartered Company Secretary in whole time practice where its paid up capital is...\n",
            "\n",
            "==================== PDF POSITION 18 ====================\n",
            "📄 Content Length: 3542 characters\n",
            "📊 Page Indicators Found: 2\n",
            "   🎯 -17- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -10- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -17 - xix) The Borrower agrees and undertakes that it shall not start implementation of the Project without obtaining IREDA's approval to designsldrawingslspecifications of the Project proposed to be ...\n",
            "\n",
            "==================== PDF POSITION 19 ====================\n",
            "📄 Content Length: 4136 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -18- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -18- xxviii) The Borrower shall agree and undertake to IREDA that if any loan andlor subsidy andlor grant andlor incentive andlor benefit islare sanctionedlgranted to the Borrower under Clean Developm...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 20 ====================\n",
            "📄 Content Length: 185 characters\n",
            "📊 Page Indicators Found: 2\n",
            "   🎯 -8- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -5- (Confidence: 77.0, Method: ARTICLE)\n",
            "📝 Content Preview:\n",
            "   -8- ARTICLE v SPECIAL CONDITIONS The Loan hereby granted shall also be subject to the Borrower complying with the special conditions and other conditions set out in Schedule VI hereto....\n",
            "   🏷️  LIKELY: Table of Contents\n",
            "\n",
            "==================== PDF POSITION 21 ====================\n",
            "📄 Content Length: 1080 characters\n",
            "📊 Page Indicators Found: 4\n",
            "   🎯 -3- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -2- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -1- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   3- ARTICLE . ! THE LOAN 2.1 AMOUNT AND TERMS QF LOAN The Borrower agrees to borrow from IREDA and IREDA agrees to lend to the Borrower, on the terms and conditions contained herein as also in the Gene...\n",
            "   🏷️  LIKELY: Table of Contents\n",
            "\n",
            "==================== PDF POSITION 22 ====================\n",
            "📄 Content Length: 2384 characters\n",
            "📊 Page Indicators Found: 4\n",
            "   🎯 -4- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "   🎯 -2- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "   🎯 -3- (Confidence: 55.0, Method: STANDALONE_NUMBER)\n",
            "📝 Content Preview:\n",
            "   -4- Further interest AlI interest which shall become due during the currency of the loan or any part thereof and for the time being remaining unpaid, and all other moneys which have become payable by ...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 23 ====================\n",
            "📄 Content Length: 56 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -11- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -11- SCHEDULE W The Project Installation of (Project No....\n",
            "\n",
            "==================== PDF POSITION 24 ====================\n",
            "📄 Content Length: 569 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -14- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -14- SCHEDULE V AMORTISATION SCHEDULE No: of Instalment Due date for Payment of Principal payment in lakhs) First Second Third Fourth Fifth Sixth Seventh Eighth Ninth Tenth Eleventh Twelfth Thirteenth...\n",
            "   🏷️  LIKELY: Cover Page\n",
            "\n",
            "==================== PDF POSITION 25 ====================\n",
            "📄 Content Length: 126 characters\n",
            "📊 Page Indicators Found: 1\n",
            "   🎯 -13- (Confidence: 99.0, Method: DASH_FORMAT)\n",
            "📝 Content Preview:\n",
            "   -13- SCHEDULE IV PARTICULARS QF INTERESI Name of the Lender Rate of Interest IREDA % plus interest tax at the applicable rate_...\n",
            "\n",
            "====================================================================================================\n",
            "📋 CREATING COMPREHENSIVE PAGE ORDER FROM ALL 25 PAGES\n",
            "====================================================================================================\n",
            "📊 COMPREHENSIVE RESULTS:\n",
            "   Total PDF pages: 25\n",
            "   Pages with numbers found: 24\n",
            "   Page numbers found: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 25, 30, 50]\n",
            "   Confidence threshold used: 25\n",
            "\n",
            "📋 FINAL ORDERING:\n",
            "   Position 1: Page -1- (PDF position 21) [Confidence: 55.0]\n",
            "   Position 2: Page -2- (PDF position 22) [Confidence: 55.0]\n",
            "   Position 3: Page -3- (PDF position 22) [Confidence: 55.0]\n",
            "   Position 4: Page -4- (PDF position 22) [Confidence: 99.0]\n",
            "   Position 5: Page -5- (PDF position 22) [Confidence: 55.0]\n",
            "   Position 6: Page -6- (PDF position 10) [Confidence: 55.0]\n",
            "   Position 7: Page -7- (PDF position 9) [Confidence: 55.0]\n",
            "   Position 8: Page -8- (PDF position 20) [Confidence: 99.0]\n",
            "   Position 9: Page -9- (PDF position 6) [Confidence: 55.0]\n",
            "   Position 10: Page -10- (PDF position 18) [Confidence: 55.0]\n",
            "   Position 11: Page -11- (PDF position 23) [Confidence: 99.0]\n",
            "   Position 12: Page -12- (PDF position 12) [Confidence: 99.0]\n",
            "   Position 13: Page -13- (PDF position 25) [Confidence: 99.0]\n",
            "   Position 14: Page -14- (PDF position 24) [Confidence: 99.0]\n",
            "   Position 15: Page -15- (PDF position 13) [Confidence: 99.0]\n",
            "   Position 16: Page -16- (PDF position 7) [Confidence: 99.0]\n",
            "   Position 17: Page -17- (PDF position 18) [Confidence: 99.0]\n",
            "   Position 18: Page -18- (PDF position 19) [Confidence: 99.0]\n",
            "   Position 19: Page -19- (PDF position 17) [Confidence: 99.0]\n",
            "   Position 20: Page -21- (PDF position 15) [Confidence: 99.0]\n",
            "   Position 21: Page -22- (PDF position 14) [Confidence: 99.0]\n",
            "   Position 22: Page -25- (PDF position 7) [Confidence: 55.0]\n",
            "   Position 23: Page -30- (PDF position 9) [Confidence: 55.0]\n",
            "   Position 24: Page -50- (PDF position 21) [Confidence: 40.0]\n",
            "\n",
            "📄 PAGES WITHOUT CLEAR NUMBERING:\n",
            "   PDF position 1: ARTICLE _ WV APPOINTMENT QF NOMNEE DIRECTORS The Borrower agrees that IREDA shall be entitled to app...\n",
            "   PDF position 2: ...\n",
            "   PDF position 3: 220- (vii) The Borrower agrees and undertakes to furnish certificate from the Chartered Company Secr...\n",
            "   PDF position 4: LOAN AGREEMENT No. Dated (Project No. Borrower Mls_ Limited Details of Project Installation of (Proj...\n",
            "   PDF position 5: LOAN AGREEMENI THIS AGREEMENT made at New Delhi, this of 20_ between Mls Limited, a Company within t...\n",
            "   PDF position 8: ~2- ARTICLE : ! DEFINITIONS GENERAL CONDITIONS 1.1 DEFINITIONS For the purpose of this Agreement the...\n",
            "   PDF position 11: LOAN AGREEMENT DATED 20_ BETWEEN MIS LIMITED AS BORROWER AND INDIAN RENEWABLE ENERGY DEVELOPMENT AGE...\n",
            "   PDF position 16: -10- SCHEDULE ! Particulars of Loan Name of the Lender Total Loan Indian Renewable Energy Rs. lakhs ...\n",
            "\n",
            "📄 Creating reordered PDF with 24 pages...\n",
            "   Position 1 ← PDF page 21\n",
            "   Position 2 ← PDF page 22\n",
            "   Position 3 ← PDF page 22\n",
            "   Position 4 ← PDF page 22\n",
            "   Position 5 ← PDF page 22\n",
            "   Position 6 ← PDF page 10\n",
            "   Position 7 ← PDF page 9\n",
            "   Position 8 ← PDF page 20\n",
            "   Position 9 ← PDF page 6\n",
            "   Position 10 ← PDF page 18\n",
            "   Position 11 ← PDF page 23\n",
            "   Position 12 ← PDF page 12\n",
            "   Position 13 ← PDF page 25\n",
            "   Position 14 ← PDF page 24\n",
            "   Position 15 ← PDF page 13\n",
            "   Position 16 ← PDF page 7\n",
            "   Position 17 ← PDF page 18\n",
            "   Position 18 ← PDF page 19\n",
            "   Position 19 ← PDF page 17\n",
            "   Position 20 ← PDF page 15\n",
            "   Position 21 ← PDF page 14\n",
            "   Position 22 ← PDF page 7\n",
            "   Position 23 ← PDF page 9\n",
            "   Position 24 ← PDF page 21\n",
            "✅ Reordered PDF saved: /content/jumbled_complete_reorder.pdf\n",
            "\n",
            "🎉 SUCCESS - ALL 25 PAGES ANALYZED!\n",
            "📁 Reordered PDF: /content/jumbled_complete_reorder.pdf\n",
            "📊 Analysis Report: /content/jumbled_25_page_analysis.json\n",
            "\n",
            "📋 FINAL SUMMARY:\n",
            "   Total pages processed: 25\n",
            "   Pages with numbers: 24\n",
            "   Pages without numbers: 8\n",
            "   Page numbers found: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 25, 30, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete PDF Analysis & Reordering System - Fixed Version (Preserves All Pages)\n",
        "!pip install pymupdf easyocr pillow numpy\n",
        "\n",
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize EasyOCR\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "class CompletePDFProcessor:\n",
        "    def __init__(self):\n",
        "        self.page_data = []\n",
        "        self.analysis_results = {}\n",
        "\n",
        "    def enhanced_page_detection(self, pdf_path):\n",
        "        \"\"\"Enhanced detection with multiple OCR strategies\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        self.page_data = []\n",
        "\n",
        "        print(f\"🔍 ANALYZING {len(doc)} pages with enhanced detection...\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "\n",
        "            print(f\"\\n📄 Processing PDF Position {page_num + 1}:\")\n",
        "\n",
        "            # Strategy 1: Native PDF text extraction\n",
        "            native_text = page.get_text()\n",
        "            native_pages = self._find_page_numbers_comprehensive(native_text, \"NATIVE\")\n",
        "\n",
        "            # Strategy 2: Standard OCR\n",
        "            standard_pages = []\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))\n",
        "                img_path = tempfile.mktemp(suffix='.png')\n",
        "                pix.save(img_path)\n",
        "\n",
        "                ocr_results = reader.readtext(img_path, detail=True)\n",
        "                ocr_text = ' '.join([text for _, text, conf in ocr_results if conf > 0.3])\n",
        "                standard_pages = self._find_page_numbers_comprehensive(ocr_text, \"STANDARD_OCR\")\n",
        "\n",
        "                os.remove(img_path)\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Standard OCR failed: {e}\")\n",
        "\n",
        "            # Strategy 3: Enhanced OCR (higher resolution + preprocessing)\n",
        "            enhanced_pages = []\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(6, 6))\n",
        "                img_path = tempfile.mktemp(suffix='.png')\n",
        "                pix.save(img_path)\n",
        "\n",
        "                img = Image.open(img_path)\n",
        "                enhancer = ImageEnhance.Contrast(img)\n",
        "                img = enhancer.enhance(2.0)\n",
        "                img = img.convert('L')\n",
        "                img = img.filter(ImageFilter.SHARPEN)\n",
        "\n",
        "                enhanced_path = tempfile.mktemp(suffix='.png')\n",
        "                img.save(enhanced_path)\n",
        "\n",
        "                ocr_results = reader.readtext(enhanced_path, detail=True, width_ths=0.3, height_ths=0.3)\n",
        "                enhanced_text = ' '.join([text for _, text, conf in ocr_results if conf > 0.2])\n",
        "                enhanced_pages = self._find_page_numbers_comprehensive(enhanced_text, \"ENHANCED_OCR\")\n",
        "\n",
        "                os.remove(img_path)\n",
        "                os.remove(enhanced_path)\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Enhanced OCR failed: {e}\")\n",
        "\n",
        "            # Strategy 4: Header region OCR\n",
        "            region_pages = []\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(4, 4))\n",
        "                img_array = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.height, pix.width, 3)\n",
        "\n",
        "                header_height = int(pix.height * 0.25)\n",
        "                header_region = img_array[:header_height, :, :]\n",
        "\n",
        "                header_img = Image.fromarray(header_region)\n",
        "                header_path = tempfile.mktemp(suffix='.png')\n",
        "                header_img.save(header_path)\n",
        "\n",
        "                header_results = reader.readtext(header_path, detail=True)\n",
        "                header_text = ' '.join([text for _, text, conf in header_results if conf > 0.2])\n",
        "                region_pages = self._find_page_numbers_comprehensive(header_text, \"HEADER_REGION\")\n",
        "\n",
        "                os.remove(header_path)\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠️ Region OCR failed: {e}\")\n",
        "\n",
        "            # Combine all findings\n",
        "            all_found_pages = native_pages + standard_pages + enhanced_pages + region_pages\n",
        "\n",
        "            # Remove duplicates, keep highest confidence\n",
        "            unique_pages = {}\n",
        "            for page_info in all_found_pages:\n",
        "                page_num_found = page_info['value']\n",
        "                if page_num_found not in unique_pages or page_info['confidence'] > unique_pages[page_num_found]['confidence']:\n",
        "                    unique_pages[page_num_found] = page_info\n",
        "\n",
        "            final_pages = list(unique_pages.values())\n",
        "            final_pages.sort(key=lambda x: x['confidence'], reverse=True)\n",
        "\n",
        "            # Show results\n",
        "            if final_pages:\n",
        "                print(f\"   ✅ Found: {[p['value'] for p in final_pages[:3]]}...\")  # Show top 3\n",
        "            else:\n",
        "                print(f\"   ❌ No page numbers detected\")\n",
        "\n",
        "            # Store page data\n",
        "            preview_text = (native_text + \" \" + (ocr_text if 'ocr_text' in locals() else \"\"))[:200].replace('\\n', ' ').strip()\n",
        "\n",
        "            page_info = {\n",
        "                'pdf_position': page_num,\n",
        "                'found_pages': final_pages,\n",
        "                'best_page': final_pages[0] if final_pages else None,\n",
        "                'text_preview': preview_text,\n",
        "                'all_text': native_text + \" \" + (enhanced_text if 'enhanced_text' in locals() else \"\")\n",
        "            }\n",
        "\n",
        "            self.page_data.append(page_info)\n",
        "\n",
        "        doc.close()\n",
        "        print(f\"\\n✅ Analysis complete: {len(self.page_data)} pages processed\")\n",
        "        return self.page_data\n",
        "\n",
        "    def _find_page_numbers_comprehensive(self, text, method_name):\n",
        "        \"\"\"Find page numbers with multiple pattern variations\"\"\"\n",
        "        found = []\n",
        "\n",
        "        if not text:\n",
        "            return found\n",
        "\n",
        "        # Enhanced patterns for different dash types and spacing\n",
        "        patterns = [\n",
        "            (r'-\\s*(\\d+)\\s*-', 90),           # -2- or - 2 -\n",
        "            (r'—\\s*(\\d+)\\s*—', 90),           # em-dash version\n",
        "            (r'–\\s*(\\d+)\\s*–', 90),           # en-dash version\n",
        "            (r'[\\-—–]\\s*(\\d+)\\s*[\\-—–]', 85), # any dash type\n",
        "            (r'^\\s*-\\s*(\\d+)\\s*-\\s*$', 95),   # -2- on own line\n",
        "            (r'page\\s+(\\d+)', 80),            # page 2\n",
        "            (r'p\\.?\\s*(\\d+)', 75),            # p. 2\n",
        "            (r'(\\d+)\\s*of\\s*\\d+', 85),       # 2 of 10\n",
        "        ]\n",
        "\n",
        "        confidence_multiplier = {'NATIVE': 1.0, 'ENHANCED_OCR': 0.9, 'STANDARD_OCR': 0.8, 'HEADER_REGION': 0.95}\n",
        "        multiplier = confidence_multiplier.get(method_name, 0.7)\n",
        "\n",
        "        for pattern, base_conf in patterns:\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
        "            for match in matches:\n",
        "                try:\n",
        "                    page_num = int(match)\n",
        "                    if 1 <= page_num <= 100:  # Reasonable range\n",
        "                        found.append({\n",
        "                            'value': page_num,\n",
        "                            'confidence': base_conf * multiplier,\n",
        "                            'method': method_name,\n",
        "                            'pattern': f'{pattern} -> {match}'\n",
        "                        })\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return found\n",
        "\n",
        "    def analyze_and_create_order(self):\n",
        "        \"\"\"Analyze findings and create optimal page order - PRESERVE ALL PAGES\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"📋 CREATING OPTIMAL PAGE ORDER (PRESERVING ALL PAGES):\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Extract high-confidence page numbers\n",
        "        page_mapping = {}\n",
        "        confidence_threshold = 50  # Adjust based on results\n",
        "\n",
        "        for page_info in self.page_data:\n",
        "            pdf_pos = page_info['pdf_position']\n",
        "            if page_info['best_page'] and page_info['best_page']['confidence'] >= confidence_threshold:\n",
        "                page_num = page_info['best_page']['value']\n",
        "                confidence = page_info['best_page']['confidence']\n",
        "                method = page_info['best_page']['method']\n",
        "\n",
        "                # Only keep if this is the best position for this page number\n",
        "                if page_num not in page_mapping or confidence > page_mapping[page_num]['confidence']:\n",
        "                    page_mapping[page_num] = {\n",
        "                        'pdf_position': pdf_pos,\n",
        "                        'confidence': confidence,\n",
        "                        'method': method\n",
        "                    }\n",
        "\n",
        "        # CREATE FINAL ORDER INCLUDING ALL PAGES\n",
        "        final_order = []\n",
        "        used_positions = set()\n",
        "\n",
        "        # STEP 1: Add numbered pages in order\n",
        "        if page_mapping:\n",
        "            sorted_pages = sorted(page_mapping.keys())\n",
        "            print(f\"Found numbered pages: {sorted_pages}\")\n",
        "\n",
        "            for page_num in sorted_pages:\n",
        "                pdf_pos = page_mapping[page_num]['pdf_position']\n",
        "                final_order.append(pdf_pos)\n",
        "                used_positions.add(pdf_pos)\n",
        "\n",
        "                info = page_mapping[page_num]\n",
        "                print(f\"Position {len(final_order)}: Page -{page_num}- (PDF pos {pdf_pos + 1}) | Conf: {info['confidence']:.1f}\")\n",
        "\n",
        "        # STEP 2: Add pages WITHOUT detected numbers at the end\n",
        "        pages_without_numbers = []\n",
        "        for page_info in self.page_data:\n",
        "            pdf_pos = page_info['pdf_position']\n",
        "            if pdf_pos not in used_positions:\n",
        "                pages_without_numbers.append({\n",
        "                    'pdf_position': pdf_pos,\n",
        "                    'text_preview': page_info['text_preview']\n",
        "                })\n",
        "\n",
        "        print(f\"\\nAdding {len(pages_without_numbers)} pages without detected numbers:\")\n",
        "        for page in pages_without_numbers:\n",
        "            pdf_pos = page['pdf_position']\n",
        "            final_order.append(pdf_pos)\n",
        "            preview = page['text_preview'][:50]\n",
        "            print(f\"Position {len(final_order)}: PDF pos {pdf_pos + 1} (No number detected) - {preview}...\")\n",
        "\n",
        "        # Analysis summary\n",
        "        total_pages = len(self.page_data)\n",
        "        numbered_pages = len(page_mapping)\n",
        "        unnumbered_pages = len(pages_without_numbers)\n",
        "\n",
        "        self.analysis_results = {\n",
        "            'total_pdf_pages': total_pages,\n",
        "            'pages_with_numbers': numbered_pages,\n",
        "            'pages_without_numbers': unnumbered_pages,\n",
        "            'page_range': f\"{min(page_mapping.keys())}-{max(page_mapping.keys())}\" if page_mapping else \"None\",\n",
        "            'confidence_threshold': confidence_threshold,\n",
        "            'final_order': final_order,\n",
        "            'page_mapping': page_mapping\n",
        "        }\n",
        "\n",
        "        print(f\"\\n📊 ANALYSIS SUMMARY:\")\n",
        "        print(f\"   Total PDF pages: {total_pages}\")\n",
        "        print(f\"   Pages with numbers: {numbered_pages}\")\n",
        "        print(f\"   Pages without numbers: {unnumbered_pages}\")\n",
        "        print(f\"   Final order length: {len(final_order)}\")\n",
        "        print(f\"   Page range found: {self.analysis_results['page_range']}\")\n",
        "\n",
        "        return final_order, self.analysis_results\n",
        "\n",
        "    def create_reordered_pdf(self, input_pdf_path, output_pdf_path, page_order):\n",
        "        \"\"\"Create reordered PDF based on analysis\"\"\"\n",
        "        if not page_order:\n",
        "            print(\"❌ Cannot create reordered PDF - no page order available\")\n",
        "            return False\n",
        "\n",
        "        print(f\"\\n📄 Creating reordered PDF...\")\n",
        "        print(f\"Input: {input_pdf_path}\")\n",
        "        print(f\"Output: {output_pdf_path}\")\n",
        "        print(f\"Order: {[pos + 1 for pos in page_order]}\")\n",
        "\n",
        "        doc = fitz.open(input_pdf_path)\n",
        "        reordered_doc = fitz.open()\n",
        "\n",
        "        for new_pos, orig_idx in enumerate(page_order):\n",
        "            reordered_doc.insert_pdf(doc, from_page=orig_idx, to_page=orig_idx)\n",
        "            print(f\"  Position {new_pos + 1} ← PDF page {orig_idx + 1}\")\n",
        "\n",
        "        reordered_doc.save(output_pdf_path)\n",
        "        doc.close()\n",
        "        reordered_doc.close()\n",
        "\n",
        "        print(f\"\\n✅ Reordered PDF created successfully!\")\n",
        "        return True\n",
        "\n",
        "    def save_analysis_report(self, output_path):\n",
        "        \"\"\"Save detailed analysis report\"\"\"\n",
        "        report_data = {\n",
        "            'analysis_results': self.analysis_results,\n",
        "            'detailed_page_data': []\n",
        "        }\n",
        "\n",
        "        for page in self.page_data:\n",
        "            page_data = {\n",
        "                'pdf_position': page['pdf_position'],\n",
        "                'found_pages': page['found_pages'],\n",
        "                'best_page': page['best_page'],\n",
        "                'text_preview': page['text_preview']\n",
        "            }\n",
        "            report_data['detailed_page_data'].append(page_data)\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(report_data, f, indent=2)\n",
        "\n",
        "        print(f\"📄 Analysis report saved: {output_path}\")\n",
        "\n",
        "    def process_pdf_complete(self, input_pdf_path, output_pdf_path=None, report_path=None):\n",
        "        \"\"\"Complete processing pipeline\"\"\"\n",
        "        print(\"🚀 STARTING COMPLETE PDF PROCESSING...\")\n",
        "\n",
        "        # Generate default output paths\n",
        "        base_name = input_pdf_path.replace('.pdf', '')\n",
        "        if not output_pdf_path:\n",
        "            output_pdf_path = f\"{base_name}_reordered.pdf\"\n",
        "        if not report_path:\n",
        "            report_path = f\"{base_name}_analysis_report.json\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Enhanced page detection\n",
        "            self.enhanced_page_detection(input_pdf_path)\n",
        "\n",
        "            # Step 2: Analyze and create order\n",
        "            page_order, analysis = self.analyze_and_create_order()\n",
        "\n",
        "            # Step 3: Create reordered PDF\n",
        "            if page_order:\n",
        "                success = self.create_reordered_pdf(input_pdf_path, output_pdf_path, page_order)\n",
        "\n",
        "                # Step 4: Save analysis report\n",
        "                self.save_analysis_report(report_path)\n",
        "\n",
        "                if success:\n",
        "                    return {\n",
        "                        'success': True,\n",
        "                        'reordered_pdf': output_pdf_path,\n",
        "                        'analysis_report': report_path,\n",
        "                        'pages_reordered': len(page_order),\n",
        "                        'analysis_summary': analysis\n",
        "                    }\n",
        "\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': 'Could not determine page order',\n",
        "                'analysis_report': report_path\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during processing: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "# USAGE EXAMPLE\n",
        "def main():\n",
        "    \"\"\"Main function to process any jumbled PDF\"\"\"\n",
        "\n",
        "    # Input PDF path - CHANGE THIS TO YOUR PDF\n",
        "    input_pdf = '/content/jumbled.pdf'  # <-- Change this path\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = CompletePDFProcessor()\n",
        "\n",
        "    # Process the PDF\n",
        "    result = processor.process_pdf_complete(input_pdf)\n",
        "\n",
        "    # Show results\n",
        "    if result['success']:\n",
        "        print(f\"\\n🎉 SUCCESS!\")\n",
        "        print(f\"📁 Reordered PDF: {result['reordered_pdf']}\")\n",
        "        print(f\"📊 Analysis Report: {result['analysis_report']}\")\n",
        "        print(f\"📄 Pages reordered: {result['pages_reordered']}\")\n",
        "\n",
        "        # Show summary\n",
        "        summary = result['analysis_summary']\n",
        "        print(f\"\\n📋 SUMMARY:\")\n",
        "        print(f\"   Total pages: {summary['total_pdf_pages']}\")\n",
        "        print(f\"   Pages with numbers: {summary['pages_with_numbers']}\")\n",
        "        print(f\"   Page range: {summary['page_range']}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n😞 FAILED: {result.get('error', 'Unknown error')}\")\n",
        "        if 'analysis_report' in result:\n",
        "            print(f\"📊 Check analysis report: {result['analysis_report']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx5G2iccnsR_",
        "outputId": "c395b41d-27a9-4b51-8e08-b767886e2477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.12/dist-packages (1.7.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.23.0+cu126)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.16.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.12/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from easyocr) (2.1.1)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from easyocr) (1.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->easyocr) (3.4.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (2025.8.28)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "🚀 STARTING COMPLETE PDF PROCESSING...\n",
            "🔍 ANALYZING 25 pages with enhanced detection...\n",
            "================================================================================\n",
            "\n",
            "📄 Processing PDF Position 1:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 2:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 3:\n",
            "   ✅ Found: [20]...\n",
            "\n",
            "📄 Processing PDF Position 4:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 5:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 6:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 7:\n",
            "   ✅ Found: [16]...\n",
            "\n",
            "📄 Processing PDF Position 8:\n",
            "   ✅ Found: [2]...\n",
            "\n",
            "📄 Processing PDF Position 9:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 10:\n",
            "   ✅ Found: [6]...\n",
            "\n",
            "📄 Processing PDF Position 11:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 12:\n",
            "   ✅ Found: [12]...\n",
            "\n",
            "📄 Processing PDF Position 13:\n",
            "   ✅ Found: [15]...\n",
            "\n",
            "📄 Processing PDF Position 14:\n",
            "   ✅ Found: [22]...\n",
            "\n",
            "📄 Processing PDF Position 15:\n",
            "   ✅ Found: [21]...\n",
            "\n",
            "📄 Processing PDF Position 16:\n",
            "   ✅ Found: [10]...\n",
            "\n",
            "📄 Processing PDF Position 17:\n",
            "   ✅ Found: [19]...\n",
            "\n",
            "📄 Processing PDF Position 18:\n",
            "   ✅ Found: [17]...\n",
            "\n",
            "📄 Processing PDF Position 19:\n",
            "   ✅ Found: [18]...\n",
            "\n",
            "📄 Processing PDF Position 20:\n",
            "   ✅ Found: [8]...\n",
            "\n",
            "📄 Processing PDF Position 21:\n",
            "   ❌ No page numbers detected\n",
            "\n",
            "📄 Processing PDF Position 22:\n",
            "   ✅ Found: [4]...\n",
            "\n",
            "📄 Processing PDF Position 23:\n",
            "   ✅ Found: [11]...\n",
            "\n",
            "📄 Processing PDF Position 24:\n",
            "   ✅ Found: [14]...\n",
            "\n",
            "📄 Processing PDF Position 25:\n",
            "   ✅ Found: [13]...\n",
            "\n",
            "✅ Analysis complete: 25 pages processed\n",
            "\n",
            "================================================================================\n",
            "📋 CREATING OPTIMAL PAGE ORDER (PRESERVING ALL PAGES):\n",
            "================================================================================\n",
            "Found numbered pages: [2, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
            "Position 1: Page -2- (PDF pos 8) | Conf: 81.0\n",
            "Position 2: Page -4- (PDF pos 22) | Conf: 85.5\n",
            "Position 3: Page -6- (PDF pos 10) | Conf: 85.5\n",
            "Position 4: Page -8- (PDF pos 20) | Conf: 85.5\n",
            "Position 5: Page -10- (PDF pos 16) | Conf: 85.5\n",
            "Position 6: Page -11- (PDF pos 23) | Conf: 85.5\n",
            "Position 7: Page -12- (PDF pos 12) | Conf: 85.5\n",
            "Position 8: Page -13- (PDF pos 25) | Conf: 85.5\n",
            "Position 9: Page -14- (PDF pos 24) | Conf: 85.5\n",
            "Position 10: Page -15- (PDF pos 13) | Conf: 85.5\n",
            "Position 11: Page -16- (PDF pos 7) | Conf: 85.5\n",
            "Position 12: Page -17- (PDF pos 18) | Conf: 85.5\n",
            "Position 13: Page -18- (PDF pos 19) | Conf: 85.5\n",
            "Position 14: Page -19- (PDF pos 17) | Conf: 85.5\n",
            "Position 15: Page -20- (PDF pos 3) | Conf: 81.0\n",
            "Position 16: Page -21- (PDF pos 15) | Conf: 85.5\n",
            "Position 17: Page -22- (PDF pos 14) | Conf: 85.5\n",
            "\n",
            "Adding 8 pages without detected numbers:\n",
            "Position 18: PDF pos 1 (No number detected) - 7- ARTICLE - IV APPOINTMENT QE NOMNEE DIRECTORS Th...\n",
            "Position 19: PDF pos 2 (No number detected) - ...\n",
            "Position 20: PDF pos 4 (No number detected) - LOANAGREEMENT No. Dated (Project No: Borrower Ms. ...\n",
            "Position 21: PDF pos 5 (No number detected) - LOANAGREEMENT THIS AGREEMENT made at New Delhi, th...\n",
            "Position 22: PDF pos 6 (No number detected) - -9 ARTICLE EFFECTIVE DATE QE AGREEMENTL PLACE QE D...\n",
            "Position 23: PDF pos 9 (No number detected) - 5- In the event of the Borrower, failing to pay th...\n",
            "Position 24: PDF pos 11 (No number detected) - LOAN AGREEMENT DATED 20_ BETWEEN MIS LIMITED AS BO...\n",
            "Position 25: PDF pos 21 (No number detected) - 3- ARTICLE - ! THELQAN 2.1 AMQUNT AND TERMS OF LOA...\n",
            "\n",
            "📊 ANALYSIS SUMMARY:\n",
            "   Total PDF pages: 25\n",
            "   Pages with numbers: 17\n",
            "   Pages without numbers: 8\n",
            "   Final order length: 25\n",
            "   Page range found: 2-22\n",
            "\n",
            "📄 Creating reordered PDF...\n",
            "Input: /content/jumbled.pdf\n",
            "Output: /content/jumbled_reordered.pdf\n",
            "Order: [8, 22, 10, 20, 16, 23, 12, 25, 24, 13, 7, 18, 19, 17, 3, 15, 14, 1, 2, 4, 5, 6, 9, 11, 21]\n",
            "  Position 1 ← PDF page 8\n",
            "  Position 2 ← PDF page 22\n",
            "  Position 3 ← PDF page 10\n",
            "  Position 4 ← PDF page 20\n",
            "  Position 5 ← PDF page 16\n",
            "  Position 6 ← PDF page 23\n",
            "  Position 7 ← PDF page 12\n",
            "  Position 8 ← PDF page 25\n",
            "  Position 9 ← PDF page 24\n",
            "  Position 10 ← PDF page 13\n",
            "  Position 11 ← PDF page 7\n",
            "  Position 12 ← PDF page 18\n",
            "  Position 13 ← PDF page 19\n",
            "  Position 14 ← PDF page 17\n",
            "  Position 15 ← PDF page 3\n",
            "  Position 16 ← PDF page 15\n",
            "  Position 17 ← PDF page 14\n",
            "  Position 18 ← PDF page 1\n",
            "  Position 19 ← PDF page 2\n",
            "  Position 20 ← PDF page 4\n",
            "  Position 21 ← PDF page 5\n",
            "  Position 22 ← PDF page 6\n",
            "  Position 23 ← PDF page 9\n",
            "  Position 24 ← PDF page 11\n",
            "  Position 25 ← PDF page 21\n",
            "\n",
            "✅ Reordered PDF created successfully!\n",
            "📄 Analysis report saved: /content/jumbled_analysis_report.json\n",
            "\n",
            "🎉 SUCCESS!\n",
            "📁 Reordered PDF: /content/jumbled_reordered.pdf\n",
            "📊 Analysis Report: /content/jumbled_analysis_report.json\n",
            "📄 Pages reordered: 25\n",
            "\n",
            "📋 SUMMARY:\n",
            "   Total pages: 25\n",
            "   Pages with numbers: 17\n",
            "   Page range: 2-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "\n",
        "# Initialize EasyOCR\n",
        "reader = easyocr.Reader(['en'], gpu=True)\n",
        "\n",
        "class PositionAwareOCR:\n",
        "    def __init__(self):\n",
        "        self.reader = reader\n",
        "\n",
        "    def extract_with_position_tolerance(self, pdf_path):\n",
        "        \"\"\"Extract page numbers with position tolerance for offset numbers\"\"\"\n",
        "        doc = fitz.open(pdf_path)\n",
        "        results = []\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc[page_num]\n",
        "\n",
        "            print(f\"\\n📄 Processing PDF position {page_num + 1}\")\n",
        "\n",
        "            # Get page dimensions\n",
        "            rect = page.rect\n",
        "            page_width = rect.width\n",
        "            page_height = rect.height\n",
        "\n",
        "            # Method 1: Standard full-page OCR\n",
        "            full_page_numbers = self.standard_ocr_extract(page)\n",
        "\n",
        "            # Method 2: Region-based OCR (scan specific areas)\n",
        "            region_numbers = self.region_based_ocr(page, page_width, page_height)\n",
        "\n",
        "            # Method 3: Multiple resolution OCR\n",
        "            multi_res_numbers = self.multi_resolution_ocr(page)\n",
        "\n",
        "            # Combine all methods\n",
        "            all_numbers = set()\n",
        "            all_numbers.update(full_page_numbers)\n",
        "            all_numbers.update(region_numbers)\n",
        "            all_numbers.update(multi_res_numbers)\n",
        "\n",
        "            # Filter reasonable page numbers\n",
        "            page_numbers = [n for n in all_numbers if 1 <= n <= 50]\n",
        "\n",
        "            print(f\"   🔢 Found numbers: {sorted(page_numbers)}\")\n",
        "\n",
        "            results.append({\n",
        "                'pdf_position': page_num,\n",
        "                'found_numbers': sorted(page_numbers),\n",
        "                'methods': {\n",
        "                    'full_page': full_page_numbers,\n",
        "                    'region_based': region_numbers,\n",
        "                    'multi_resolution': multi_res_numbers\n",
        "                }\n",
        "            })\n",
        "\n",
        "        doc.close()\n",
        "        return results\n",
        "\n",
        "    def standard_ocr_extract(self, page):\n",
        "        \"\"\"Standard full-page OCR\"\"\"\n",
        "        try:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(4, 4))\n",
        "            img_path = tempfile.mktemp(suffix='.png')\n",
        "            pix.save(img_path)\n",
        "\n",
        "            # Enhanced preprocessing\n",
        "            img = Image.open(img_path)\n",
        "            enhancer = ImageEnhance.Contrast(img)\n",
        "            img = enhancer.enhance(1.5)\n",
        "            img = img.convert('L')\n",
        "\n",
        "            enhanced_path = tempfile.mktemp(suffix='.png')\n",
        "            img.save(enhanced_path)\n",
        "\n",
        "            # OCR with lower confidence threshold\n",
        "            ocr_results = self.reader.readtext(enhanced_path, detail=True)\n",
        "\n",
        "            # Extract all numbers\n",
        "            numbers = []\n",
        "            for bbox, text, conf in ocr_results:\n",
        "                # Try to find numbers in the text\n",
        "                found_nums = re.findall(r'\\b(\\d+)\\b', text)\n",
        "                for num_str in found_nums:\n",
        "                    if num_str.isdigit():\n",
        "                        numbers.append(int(num_str))\n",
        "\n",
        "            os.remove(img_path)\n",
        "            os.remove(enhanced_path)\n",
        "\n",
        "            return numbers\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Standard OCR failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def region_based_ocr(self, page, page_width, page_height):\n",
        "        \"\"\"OCR specific regions where page numbers typically appear\"\"\"\n",
        "        numbers = []\n",
        "\n",
        "        # Define regions to scan (top/bottom margins, left/right margins)\n",
        "        regions = [\n",
        "            # Top margin (full width)\n",
        "            fitz.Rect(0, 0, page_width, page_height * 0.1),\n",
        "            # Bottom margin (full width)\n",
        "            fitz.Rect(0, page_height * 0.9, page_width, page_height),\n",
        "            # Left margin (full height)\n",
        "            fitz.Rect(0, 0, page_width * 0.1, page_height),\n",
        "            # Right margin (full height)\n",
        "            fitz.Rect(page_width * 0.9, 0, page_width, page_height),\n",
        "            # Center bottom (common page number location)\n",
        "            fitz.Rect(page_width * 0.3, page_height * 0.85, page_width * 0.7, page_height),\n",
        "            # Center top\n",
        "            fitz.Rect(page_width * 0.3, 0, page_width * 0.7, page_height * 0.15)\n",
        "        ]\n",
        "\n",
        "        for i, region in enumerate(regions):\n",
        "            try:\n",
        "                # Extract this region as image\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(6, 6), clip=region)  # Higher resolution\n",
        "\n",
        "                if pix.width > 10 and pix.height > 10:  # Skip tiny regions\n",
        "                    img_path = tempfile.mktemp(suffix='.png')\n",
        "                    pix.save(img_path)\n",
        "\n",
        "                    # Enhanced preprocessing for small regions\n",
        "                    img = Image.open(img_path)\n",
        "\n",
        "                    # More aggressive enhancement for small regions\n",
        "                    enhancer = ImageEnhance.Contrast(img)\n",
        "                    img = enhancer.enhance(2.5)\n",
        "                    enhancer = ImageEnhance.Sharpness(img)\n",
        "                    img = enhancer.enhance(1.5)\n",
        "                    img = img.convert('L')\n",
        "\n",
        "                    enhanced_path = tempfile.mktemp(suffix='.png')\n",
        "                    img.save(enhanced_path)\n",
        "\n",
        "                    # OCR with very low confidence threshold for regions\n",
        "                    ocr_results = self.reader.readtext(enhanced_path, detail=True)\n",
        "\n",
        "                    for bbox, text, conf in ocr_results:\n",
        "                        # Even accept low confidence for isolated numbers\n",
        "                        found_nums = re.findall(r'\\b(\\d+)\\b', text.strip())\n",
        "                        for num_str in found_nums:\n",
        "                            if num_str.isdigit() and len(num_str) <= 2:  # 1-2 digit numbers\n",
        "                                numbers.append(int(num_str))\n",
        "                                print(f\"     🎯 Region {i} found: {num_str} (conf: {conf:.2f})\")\n",
        "\n",
        "                    os.remove(img_path)\n",
        "                    os.remove(enhanced_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return list(set(numbers))  # Remove duplicates\n",
        "\n",
        "    def multi_resolution_ocr(self, page):\n",
        "        \"\"\"Try OCR at multiple resolutions to catch different text sizes\"\"\"\n",
        "        numbers = []\n",
        "\n",
        "        # Try different zoom levels\n",
        "        zoom_levels = [2, 3, 4, 6, 8]  # Different resolutions\n",
        "\n",
        "        for zoom in zoom_levels:\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))\n",
        "                img_path = tempfile.mktemp(suffix='.png')\n",
        "                pix.save(img_path)\n",
        "\n",
        "                # Minimal preprocessing to preserve text\n",
        "                img = Image.open(img_path)\n",
        "                img = img.convert('L')\n",
        "\n",
        "                # OCR at this resolution\n",
        "                ocr_results = self.reader.readtext(img_path, detail=True)\n",
        "\n",
        "                for bbox, text, conf in ocr_results:\n",
        "                    # Look for standalone numbers or page patterns\n",
        "                    patterns = [\n",
        "                        r'^\\s*(\\d+)\\s*$',  # Isolated number\n",
        "                        r'-\\s*(\\d+)\\s*-',   # -7-\n",
        "                        r'page\\s*(\\d+)',    # page 7\n",
        "                        r'(\\d+)\\s*$'        # Number at end of line\n",
        "                    ]\n",
        "\n",
        "                    for pattern in patterns:\n",
        "                        matches = re.findall(pattern, text.strip(), re.IGNORECASE)\n",
        "                        for match in matches:\n",
        "                            if match.isdigit():\n",
        "                                num = int(match)\n",
        "                                if 1 <= num <= 50:  # Reasonable page range\n",
        "                                    numbers.append(num)\n",
        "                                    print(f\"     🔍 Zoom {zoom}x found: {num} (pattern: {pattern[:10]}...)\")\n",
        "\n",
        "                os.remove(img_path)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return list(set(numbers))\n",
        "\n",
        "    def create_comprehensive_mapping(self, results):\n",
        "        \"\"\"Create comprehensive page mapping from position-aware results\"\"\"\n",
        "        print(f\"\\n{'='*60} POSITION-AWARE RESULTS {'='*60}\")\n",
        "\n",
        "        # Collect all found page numbers with their positions\n",
        "        page_to_position = {}\n",
        "        position_to_pages = {}\n",
        "\n",
        "        for result in results:\n",
        "            pdf_pos = result['pdf_position']\n",
        "            found_numbers = result['found_numbers']\n",
        "\n",
        "            position_to_pages[pdf_pos] = found_numbers\n",
        "\n",
        "            for page_num in found_numbers:\n",
        "                if page_num not in page_to_position:\n",
        "                    page_to_position[page_num] = []\n",
        "                page_to_position[page_num].append(pdf_pos)\n",
        "\n",
        "        print(\"📊 COMPREHENSIVE FINDINGS:\")\n",
        "        print(f\"   Total PDF positions scanned: {len(results)}\")\n",
        "        print(f\"   Positions with page numbers: {len([r for r in results if r['found_numbers']])}\")\n",
        "\n",
        "        # Show what we found\n",
        "        print(f\"\\n📋 PAGE NUMBERS FOUND:\")\n",
        "        for page_num in sorted(page_to_position.keys()):\n",
        "            positions = page_to_position[page_num]\n",
        "            print(f\"   Page {page_num}: Found at PDF position(s) {positions}\")\n",
        "\n",
        "        # Show positions without clear numbers\n",
        "        print(f\"\\n📄 PDF POSITIONS WITHOUT CLEAR PAGE NUMBERS:\")\n",
        "        for result in results:\n",
        "            if not result['found_numbers']:\n",
        "                print(f\"   PDF position {result['pdf_position'] + 1}: No clear page numbers\")\n",
        "\n",
        "        # Try to create best mapping (one page number per position)\n",
        "        best_mapping = {}\n",
        "        for result in results:\n",
        "            pdf_pos = result['pdf_position']\n",
        "            found_numbers = result['found_numbers']\n",
        "\n",
        "            if len(found_numbers) == 1:\n",
        "                # Clear single match\n",
        "                best_mapping[found_numbers[0]] = pdf_pos\n",
        "            elif len(found_numbers) > 1:\n",
        "                # Multiple matches - need heuristic\n",
        "                # Prefer numbers that appear uniquely\n",
        "                unique_nums = [n for n in found_numbers if len(page_to_position[n]) == 1]\n",
        "                if unique_nums:\n",
        "                    best_mapping[unique_nums[0]] = pdf_pos\n",
        "\n",
        "        return best_mapping, page_to_position\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def main():\n",
        "    \"\"\"Run position-aware analysis\"\"\"\n",
        "    input_pdf = '/content/jumbled.pdf'\n",
        "\n",
        "    analyzer = PositionAwareOCR()\n",
        "    results = analyzer.extract_with_position_tolerance(input_pdf)\n",
        "    best_mapping, all_findings = analyzer.create_comprehensive_mapping(results)\n",
        "\n",
        "    print(f\"\\n🎯 TARGETING MISSING PAGES [7, 9, 5, 3]:\")\n",
        "    target_pages = [7, 9, 5, 3]\n",
        "\n",
        "    for target in target_pages:\n",
        "        if target in all_findings:\n",
        "            positions = all_findings[target]\n",
        "            print(f\"   ✅ Page {target}: FOUND at PDF position(s) {[p+1 for p in positions]}\")\n",
        "        else:\n",
        "            print(f\"   ❌ Page {target}: Still not found\")\n",
        "\n",
        "    return results, best_mapping\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XccktfRH1txt",
        "outputId": "6525c643-5e68-474a-8d98-1cbdd4db14f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Processing PDF position 1\n",
            "   🔢 Found numbers: []\n",
            "\n",
            "📄 Processing PDF position 2\n",
            "   🔢 Found numbers: []\n",
            "\n",
            "📄 Processing PDF position 3\n",
            "     🎯 Region 0 found: 20 (conf: 0.32)\n",
            "     🎯 Region 5 found: 20 (conf: 0.81)\n",
            "     🔍 Zoom 3x found: 1 (pattern: ^\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 20 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 20 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 20 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [1, 20]\n",
            "\n",
            "📄 Processing PDF position 4\n",
            "     🔍 Zoom 2x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [3]\n",
            "\n",
            "📄 Processing PDF position 5\n",
            "     🔍 Zoom 8x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [1, 3]\n",
            "\n",
            "📄 Processing PDF position 6\n",
            "     🎯 Region 0 found: 9 (conf: 0.93)\n",
            "     🎯 Region 5 found: 9 (conf: 0.97)\n",
            "     🔍 Zoom 3x found: 9 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 9 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 9 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 9 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [9]\n",
            "\n",
            "📄 Processing PDF position 7\n",
            "     🎯 Region 0 found: 16 (conf: 0.90)\n",
            "     🎯 Region 5 found: 16 (conf: 0.98)\n",
            "     🔍 Zoom 3x found: 16 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 16 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 16 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 16 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [16, 25]\n",
            "\n",
            "📄 Processing PDF position 8\n",
            "     🎯 Region 0 found: 2 (conf: 0.95)\n",
            "     🎯 Region 5 found: 2 (conf: 0.97)\n",
            "     🔍 Zoom 2x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 2 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 22 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 2 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 22 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [1, 2, 22]\n",
            "\n",
            "📄 Processing PDF position 9\n",
            "     🎯 Region 0 found: 5 (conf: 0.99)\n",
            "     🎯 Region 5 found: 5 (conf: 0.81)\n",
            "     🔍 Zoom 2x found: 6 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 7 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 8 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 6 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 7 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 8 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 6 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 7 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 8 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 6 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 7 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 8 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 6 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 7 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 8 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [2, 5, 6, 7, 8, 30]\n",
            "\n",
            "📄 Processing PDF position 10\n",
            "     🎯 Region 0 found: 6 (conf: 1.00)\n",
            "     🎯 Region 5 found: 6 (conf: 0.91)\n",
            "     🔍 Zoom 2x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 6 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 6 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [1, 3, 6]\n",
            "\n",
            "📄 Processing PDF position 11\n",
            "   🔢 Found numbers: []\n",
            "\n",
            "📄 Processing PDF position 12\n",
            "     🎯 Region 0 found: 12 (conf: 0.93)\n",
            "     🎯 Region 5 found: 12 (conf: 0.97)\n",
            "     🔍 Zoom 3x found: 12 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 12 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 12 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 12 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [12]\n",
            "\n",
            "📄 Processing PDF position 13\n",
            "     🎯 Region 0 found: 15 (conf: 0.43)\n",
            "     🎯 Region 5 found: 15 (conf: 0.99)\n",
            "     🔍 Zoom 3x found: 15 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 15 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 15 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 15 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [15]\n",
            "\n",
            "📄 Processing PDF position 14\n",
            "     🎯 Region 0 found: 22 (conf: 0.37)\n",
            "     🎯 Region 5 found: 22 (conf: 0.91)\n",
            "     🔍 Zoom 4x found: 22 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 22 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 22 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [22]\n",
            "\n",
            "📄 Processing PDF position 15\n",
            "     🎯 Region 0 found: 21 (conf: 0.98)\n",
            "     🎯 Region 5 found: 21 (conf: 0.98)\n",
            "     🔍 Zoom 2x found: 21 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 21 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 21 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 21 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 21 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [2, 3, 21]\n",
            "\n",
            "📄 Processing PDF position 16\n",
            "     🎯 Region 0 found: 10 (conf: 0.65)\n",
            "     🎯 Region 5 found: 10 (conf: 0.60)\n",
            "     🔍 Zoom 3x found: 10 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 10 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 10 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [10]\n",
            "\n",
            "📄 Processing PDF position 17\n",
            "     🎯 Region 0 found: 19 (conf: 0.99)\n",
            "     🎯 Region 5 found: 19 (conf: 1.00)\n",
            "     🔍 Zoom 3x found: 19 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 19 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 19 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 19 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [1, 3, 5, 10, 19, 50]\n",
            "\n",
            "📄 Processing PDF position 18\n",
            "     🎯 Region 0 found: 17 (conf: 1.00)\n",
            "     🎯 Region 5 found: 17 (conf: 0.97)\n",
            "     🔍 Zoom 2x found: 17 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 17 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 17 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 17 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 17 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [10, 17]\n",
            "\n",
            "📄 Processing PDF position 19\n",
            "     🎯 Region 0 found: 18 (conf: 0.94)\n",
            "     🎯 Region 5 found: 18 (conf: 0.99)\n",
            "     🔍 Zoom 3x found: 18 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 18 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 18 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 18 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [18]\n",
            "\n",
            "📄 Processing PDF position 20\n",
            "     🎯 Region 0 found: 8 (conf: 0.97)\n",
            "     🎯 Region 5 found: 8 (conf: 1.00)\n",
            "     🔍 Zoom 3x found: 8 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 3 (pattern: ^\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 8 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 3 (pattern: ^\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 8 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 3 (pattern: ^\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 8 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [3, 8]\n",
            "\n",
            "📄 Processing PDF position 21\n",
            "     🎯 Region 0 found: 3 (conf: 0.93)\n",
            "     🎯 Region 5 found: 3 (conf: 0.53)\n",
            "     🔍 Zoom 2x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 1 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 2 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [1, 2, 3, 4, 50]\n",
            "\n",
            "📄 Processing PDF position 22\n",
            "     🎯 Region 0 found: 4 (conf: 0.93)\n",
            "     🎯 Region 5 found: 4 (conf: 0.99)\n",
            "     🔍 Zoom 2x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 2x found: 5 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 4 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 3x found: 5 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 4 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 4x found: 5 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 4 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 6x found: 5 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 4 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 3 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 4 (pattern: (\\d+)\\s*$...)\n",
            "     🔍 Zoom 8x found: 5 (pattern: (\\d+)\\s*$...)\n",
            "   🔢 Found numbers: [2, 3, 4, 5]\n",
            "\n",
            "📄 Processing PDF position 23\n",
            "     🎯 Region 0 found: 11 (conf: 0.98)\n",
            "     🎯 Region 5 found: 11 (conf: 0.79)\n",
            "     🔍 Zoom 2x found: 11 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 11 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 11 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 11 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 11 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [11]\n",
            "\n",
            "📄 Processing PDF position 24\n",
            "     🎯 Region 0 found: 14 (conf: 0.63)\n",
            "     🎯 Region 5 found: 14 (conf: 0.99)\n",
            "     🔍 Zoom 2x found: 14 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 3x found: 14 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 14 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 14 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 14 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [14]\n",
            "\n",
            "📄 Processing PDF position 25\n",
            "     🎯 Region 0 found: 13 (conf: 0.91)\n",
            "     🎯 Region 5 found: 13 (conf: 0.48)\n",
            "     🔍 Zoom 3x found: 13 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 4x found: 13 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 6x found: 13 (pattern: -\\s*(\\d+)\\...)\n",
            "     🔍 Zoom 8x found: 13 (pattern: -\\s*(\\d+)\\...)\n",
            "   🔢 Found numbers: [13]\n",
            "\n",
            "============================================================ POSITION-AWARE RESULTS ============================================================\n",
            "📊 COMPREHENSIVE FINDINGS:\n",
            "   Total PDF positions scanned: 25\n",
            "   Positions with page numbers: 22\n",
            "\n",
            "📋 PAGE NUMBERS FOUND:\n",
            "   Page 1: Found at PDF position(s) [2, 4, 7, 9, 16, 20]\n",
            "   Page 2: Found at PDF position(s) [7, 8, 14, 20, 21]\n",
            "   Page 3: Found at PDF position(s) [3, 4, 9, 14, 16, 19, 20, 21]\n",
            "   Page 4: Found at PDF position(s) [20, 21]\n",
            "   Page 5: Found at PDF position(s) [8, 16, 21]\n",
            "   Page 6: Found at PDF position(s) [8, 9]\n",
            "   Page 7: Found at PDF position(s) [8]\n",
            "   Page 8: Found at PDF position(s) [8, 19]\n",
            "   Page 9: Found at PDF position(s) [5]\n",
            "   Page 10: Found at PDF position(s) [15, 16, 17]\n",
            "   Page 11: Found at PDF position(s) [22]\n",
            "   Page 12: Found at PDF position(s) [11]\n",
            "   Page 13: Found at PDF position(s) [24]\n",
            "   Page 14: Found at PDF position(s) [23]\n",
            "   Page 15: Found at PDF position(s) [12]\n",
            "   Page 16: Found at PDF position(s) [6]\n",
            "   Page 17: Found at PDF position(s) [17]\n",
            "   Page 18: Found at PDF position(s) [18]\n",
            "   Page 19: Found at PDF position(s) [16]\n",
            "   Page 20: Found at PDF position(s) [2]\n",
            "   Page 21: Found at PDF position(s) [14]\n",
            "   Page 22: Found at PDF position(s) [7, 13]\n",
            "   Page 25: Found at PDF position(s) [6]\n",
            "   Page 30: Found at PDF position(s) [8]\n",
            "   Page 50: Found at PDF position(s) [16, 20]\n",
            "\n",
            "📄 PDF POSITIONS WITHOUT CLEAR PAGE NUMBERS:\n",
            "   PDF position 1: No clear page numbers\n",
            "   PDF position 2: No clear page numbers\n",
            "   PDF position 11: No clear page numbers\n",
            "\n",
            "🎯 TARGETING MISSING PAGES [7, 9, 5, 3]:\n",
            "   ✅ Page 7: FOUND at PDF position(s) [9]\n",
            "   ✅ Page 9: FOUND at PDF position(s) [6]\n",
            "   ✅ Page 5: FOUND at PDF position(s) [9, 17, 22]\n",
            "   ✅ Page 3: FOUND at PDF position(s) [4, 5, 10, 15, 17, 20, 21, 22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "import easyocr\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import argparse\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageEnhance\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import torch\n",
        "\n",
        "class AdaptivePDFReorderer:\n",
        "    def __init__(self, gpu_optimization=True, max_workers=None):\n",
        "        self.setup_logging()\n",
        "        self.gpu_optimization = gpu_optimization\n",
        "        self.max_workers = max_workers or min(8, os.cpu_count())\n",
        "\n",
        "        # Initialize EasyOCR with GPU optimization\n",
        "        self.reader = self._initialize_ocr()\n",
        "\n",
        "        # Analysis results\n",
        "        self.page_findings = {}\n",
        "        self.confidence_scores = {}\n",
        "        self.disambiguation_data = {}\n",
        "\n",
        "        self.logger.info(f\"Initialized with GPU optimization: {gpu_optimization}\")\n",
        "        self.logger.info(f\"Max workers: {self.max_workers}\")\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _initialize_ocr(self):\n",
        "        try:\n",
        "            if self.gpu_optimization and torch.cuda.is_available():\n",
        "                # Optimize GPU memory usage\n",
        "                torch.cuda.empty_cache()\n",
        "                reader = easyocr.Reader(['en'], gpu=True, verbose=False)\n",
        "                self.logger.info(f\"GPU initialized. CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
        "            else:\n",
        "                reader = easyocr.Reader(['en'], gpu=False, verbose=False)\n",
        "                self.logger.info(\"Using CPU for OCR\")\n",
        "            return reader\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"OCR initialization failed: {e}\")\n",
        "            return easyocr.Reader(['en'], gpu=False, verbose=False)\n",
        "\n",
        "    def analyze_pdf(self, pdf_path):\n",
        "        \"\"\"Comprehensive PDF analysis with adaptive page detection\"\"\"\n",
        "        if not Path(pdf_path).exists():\n",
        "            raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "        total_pages = len(doc)\n",
        "\n",
        "        self.logger.info(f\"Analyzing PDF: {pdf_path}\")\n",
        "        self.logger.info(f\"Total pages: {total_pages}\")\n",
        "\n",
        "        # Parallel processing for speed\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "            futures = []\n",
        "\n",
        "            for page_num in range(total_pages):\n",
        "                future = executor.submit(self._analyze_single_page, doc, page_num)\n",
        "                futures.append((page_num, future))\n",
        "\n",
        "            # Collect results\n",
        "            for page_num, future in futures:\n",
        "                try:\n",
        "                    result = future.result(timeout=30)\n",
        "                    if result:\n",
        "                        self.page_findings[page_num] = result\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Failed to analyze page {page_num + 1}: {e}\")\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "        # Clean up GPU memory\n",
        "        if self.gpu_optimization:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        self.logger.info(f\"Analysis complete. Found page numbers on {len(self.page_findings)} pages\")\n",
        "        return self._create_page_mapping()\n",
        "\n",
        "    def _analyze_single_page(self, doc, page_num):\n",
        "        \"\"\"Analyze single page with multiple OCR approaches\"\"\"\n",
        "        page = doc[page_num]\n",
        "\n",
        "        # Get page dimensions for region-based analysis\n",
        "        rect = page.rect\n",
        "        page_width, page_height = rect.width, rect.height\n",
        "\n",
        "        all_found_numbers = set()\n",
        "        detection_methods = {}\n",
        "\n",
        "        try:\n",
        "            # Method 1: Standard full-page OCR\n",
        "            full_page_numbers = self._standard_ocr(page)\n",
        "            all_found_numbers.update(full_page_numbers)\n",
        "            if full_page_numbers:\n",
        "                detection_methods['full_page'] = full_page_numbers\n",
        "\n",
        "            # Method 2: Region-based OCR (header/footer areas)\n",
        "            region_numbers = self._region_based_ocr(page, page_width, page_height)\n",
        "            all_found_numbers.update(region_numbers)\n",
        "            if region_numbers:\n",
        "                detection_methods['regions'] = region_numbers\n",
        "\n",
        "            # Method 3: Multi-resolution OCR\n",
        "            multi_res_numbers = self._multi_resolution_ocr(page)\n",
        "            all_found_numbers.update(multi_res_numbers)\n",
        "            if multi_res_numbers:\n",
        "                detection_methods['multi_res'] = multi_res_numbers\n",
        "\n",
        "            # Filter to reasonable page numbers\n",
        "            valid_numbers = [n for n in all_found_numbers if 1 <= n <= 200]\n",
        "\n",
        "            if valid_numbers:\n",
        "                return {\n",
        "                    'found_numbers': sorted(valid_numbers),\n",
        "                    'detection_methods': detection_methods,\n",
        "                    'primary_candidates': self._identify_primary_candidates(valid_numbers, detection_methods)\n",
        "                }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.debug(f\"Error analyzing page {page_num + 1}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _standard_ocr(self, page):\n",
        "        \"\"\"Standard OCR approach\"\"\"\n",
        "        try:\n",
        "            pix = page.get_pixmap(matrix=fitz.Matrix(3, 3))  # Optimized resolution\n",
        "            img_path = tempfile.mktemp(suffix='.png')\n",
        "            pix.save(img_path)\n",
        "\n",
        "            # Preprocess image\n",
        "            img = Image.open(img_path)\n",
        "            enhancer = ImageEnhance.Contrast(img)\n",
        "            img = enhancer.enhance(1.3)\n",
        "            img = img.convert('L')\n",
        "\n",
        "            # OCR\n",
        "            results = self.reader.readtext(img_path, detail=True)\n",
        "            numbers = self._extract_numbers_from_ocr(results)\n",
        "\n",
        "            os.remove(img_path)\n",
        "            return numbers\n",
        "\n",
        "        except Exception:\n",
        "            return []\n",
        "\n",
        "    def _region_based_ocr(self, page, page_width, page_height):\n",
        "        \"\"\"OCR specific regions where page numbers typically appear\"\"\"\n",
        "        numbers = set()\n",
        "\n",
        "        # Define key regions\n",
        "        regions = [\n",
        "            fitz.Rect(0, 0, page_width, page_height * 0.08),  # Top header\n",
        "            fitz.Rect(0, page_height * 0.92, page_width, page_height),  # Bottom footer\n",
        "            fitz.Rect(page_width * 0.4, page_height * 0.9, page_width * 0.6, page_height),  # Center bottom\n",
        "        ]\n",
        "\n",
        "        for region in regions:\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(4, 4), clip=region)\n",
        "\n",
        "                if pix.width > 20 and pix.height > 10:\n",
        "                    img_path = tempfile.mktemp(suffix='.png')\n",
        "                    pix.save(img_path)\n",
        "\n",
        "                    # Enhanced preprocessing for small regions\n",
        "                    img = Image.open(img_path)\n",
        "                    enhancer = ImageEnhance.Contrast(img)\n",
        "                    img = enhancer.enhance(2.0)\n",
        "                    img = img.convert('L')\n",
        "\n",
        "                    results = self.reader.readtext(img_path, detail=True)\n",
        "                    region_numbers = self._extract_numbers_from_ocr(results, min_confidence=0.3)\n",
        "                    numbers.update(region_numbers)\n",
        "\n",
        "                    os.remove(img_path)\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return list(numbers)\n",
        "\n",
        "    def _multi_resolution_ocr(self, page):\n",
        "        \"\"\"OCR at multiple resolutions\"\"\"\n",
        "        numbers = set()\n",
        "        resolutions = [2, 4, 6]  # Optimized resolution set\n",
        "\n",
        "        for zoom in resolutions:\n",
        "            try:\n",
        "                pix = page.get_pixmap(matrix=fitz.Matrix(zoom, zoom))\n",
        "                img_path = tempfile.mktemp(suffix='.png')\n",
        "                pix.save(img_path)\n",
        "\n",
        "                results = self.reader.readtext(img_path, detail=True)\n",
        "                res_numbers = self._extract_numbers_from_ocr(results, min_confidence=0.4)\n",
        "                numbers.update(res_numbers)\n",
        "\n",
        "                os.remove(img_path)\n",
        "\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return list(numbers)\n",
        "\n",
        "    def _extract_numbers_from_ocr(self, ocr_results, min_confidence=0.5):\n",
        "        \"\"\"Extract page numbers from OCR results using multiple patterns\"\"\"\n",
        "        numbers = set()\n",
        "\n",
        "        patterns = [\n",
        "            (r'^\\s*(\\d+)\\s*$', 1.0),          # Isolated number (high confidence)\n",
        "            (r'-\\s*(\\d+)\\s*-', 0.9),          # Dash format\n",
        "            (r'page\\s*(\\d+)', 0.8),           # \"Page N\"\n",
        "            (r'(\\d+)\\s*$', 0.7),              # Number at line end\n",
        "            (r'^\\s*(\\d+)', 0.6),              # Number at line start\n",
        "        ]\n",
        "\n",
        "        for bbox, text, confidence in ocr_results:\n",
        "            if confidence < min_confidence:\n",
        "                continue\n",
        "\n",
        "            text_clean = text.strip()\n",
        "\n",
        "            for pattern, pattern_weight in patterns:\n",
        "                matches = re.findall(pattern, text_clean, re.IGNORECASE)\n",
        "                for match in matches:\n",
        "                    if match.isdigit():\n",
        "                        num = int(match)\n",
        "                        if 1 <= num <= 200:  # Reasonable page range\n",
        "                            numbers.add(num)\n",
        "\n",
        "        return list(numbers)\n",
        "\n",
        "    def _identify_primary_candidates(self, numbers, methods):\n",
        "        \"\"\"Identify the most likely page number candidates\"\"\"\n",
        "        # Count detection frequency across methods\n",
        "        frequency = Counter()\n",
        "        for method_numbers in methods.values():\n",
        "            frequency.update(method_numbers)\n",
        "\n",
        "        # Return numbers detected by multiple methods or with high confidence\n",
        "        primary = []\n",
        "        for num, count in frequency.most_common():\n",
        "            if count >= 2 or len([m for m in methods.values() if num in m]) >= 2:\n",
        "                primary.append(num)\n",
        "\n",
        "        return primary[:3]  # Top 3 candidates\n",
        "\n",
        "    def _create_page_mapping(self):\n",
        "        \"\"\"Create intelligent page number to position mapping\"\"\"\n",
        "        self.logger.info(\"Creating page mapping with disambiguation...\")\n",
        "\n",
        "        # Collect all page number detections\n",
        "        detections = defaultdict(list)\n",
        "        for pdf_pos, data in self.page_findings.items():\n",
        "            for page_num in data['primary_candidates']:\n",
        "                detections[page_num].append(pdf_pos)\n",
        "\n",
        "        # Resolve conflicts using heuristics\n",
        "        final_mapping = {}\n",
        "        used_positions = set()\n",
        "\n",
        "        # Sort page numbers for sequential processing\n",
        "        for page_num in sorted(detections.keys()):\n",
        "            positions = detections[page_num]\n",
        "\n",
        "            # Filter out already used positions\n",
        "            available_positions = [p for p in positions if p not in used_positions]\n",
        "\n",
        "            if available_positions:\n",
        "                # Choose best position using heuristics\n",
        "                best_pos = self._choose_best_position(page_num, available_positions)\n",
        "                final_mapping[page_num] = best_pos\n",
        "                used_positions.add(best_pos)\n",
        "\n",
        "        self.logger.info(f\"Final mapping created: {len(final_mapping)} pages mapped\")\n",
        "\n",
        "        # Log mapping for verification\n",
        "        for page_num in sorted(final_mapping.keys()):\n",
        "            pdf_pos = final_mapping[page_num]\n",
        "            self.logger.info(f\"Page {page_num} -> PDF position {pdf_pos + 1}\")\n",
        "\n",
        "        return final_mapping\n",
        "\n",
        "    def _choose_best_position(self, page_num, positions):\n",
        "        \"\"\"Choose the best PDF position for a page number using heuristics\"\"\"\n",
        "        if len(positions) == 1:\n",
        "            return positions[0]\n",
        "\n",
        "        # Heuristic: prefer positions that have fewer competing page numbers\n",
        "        position_scores = {}\n",
        "\n",
        "        for pos in positions:\n",
        "            score = 0\n",
        "\n",
        "            # Prefer positions with fewer total detections\n",
        "            total_detections = len(self.page_findings[pos]['found_numbers'])\n",
        "            score += 1.0 / (total_detections + 1)\n",
        "\n",
        "            # Prefer positions where this page number is a primary candidate\n",
        "            if page_num in self.page_findings[pos]['primary_candidates']:\n",
        "                score += 1.0\n",
        "\n",
        "            # Prefer positions that are reasonable for the page number\n",
        "            expected_range_start = max(0, page_num - 3)\n",
        "            expected_range_end = min(len(self.page_findings), page_num + 3)\n",
        "            if expected_range_start <= pos <= expected_range_end:\n",
        "                score += 0.5\n",
        "\n",
        "            position_scores[pos] = score\n",
        "\n",
        "        return max(positions, key=lambda p: position_scores.get(p, 0))\n",
        "\n",
        "    def create_reordered_pdf(self, input_path, output_path, page_mapping):\n",
        "        \"\"\"Create reordered PDF based on page mapping\"\"\"\n",
        "        if not page_mapping:\n",
        "            raise ValueError(\"No page mapping available for reordering\")\n",
        "\n",
        "        self.logger.info(f\"Creating reordered PDF: {output_path}\")\n",
        "\n",
        "        doc = fitz.open(input_path)\n",
        "        reordered_doc = fitz.open()\n",
        "\n",
        "        # Sort by page numbers and reorder\n",
        "        sorted_pages = sorted(page_mapping.items())\n",
        "\n",
        "        for page_num, pdf_position in sorted_pages:\n",
        "            reordered_doc.insert_pdf(doc, from_page=pdf_position, to_page=pdf_position)\n",
        "            self.logger.debug(f\"Added page {page_num} from PDF position {pdf_position + 1}\")\n",
        "\n",
        "        # Save reordered PDF\n",
        "        reordered_doc.save(output_path)\n",
        "        doc.close()\n",
        "        reordered_doc.close()\n",
        "\n",
        "        self.logger.info(f\"Reordered PDF saved: {output_path}\")\n",
        "        self.logger.info(f\"Pages reordered: {len(sorted_pages)}\")\n",
        "\n",
        "        return len(sorted_pages)\n",
        "\n",
        "    def generate_report(self, output_path, page_mapping):\n",
        "        \"\"\"Generate detailed analysis report\"\"\"\n",
        "        report = {\n",
        "            'summary': {\n",
        "                'total_pdf_pages': len(self.page_findings) if hasattr(self, 'page_findings') else 0,\n",
        "                'pages_with_numbers': len([p for p in self.page_findings.values() if p['found_numbers']]),\n",
        "                'successful_mappings': len(page_mapping),\n",
        "                'page_range': f\"{min(page_mapping.keys())}-{max(page_mapping.keys())}\" if page_mapping else \"None\"\n",
        "            },\n",
        "            'page_mappings': page_mapping,\n",
        "            'detailed_findings': self.page_findings\n",
        "        }\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        self.logger.info(f\"Analysis report saved: {output_path}\")\n",
        "\n",
        "    def process_pdf(self, input_path, output_dir=None):\n",
        "        \"\"\"Complete PDF processing pipeline\"\"\"\n",
        "        input_path = Path(input_path)\n",
        "\n",
        "        if not input_path.exists():\n",
        "            raise FileNotFoundError(f\"Input PDF not found: {input_path}\")\n",
        "\n",
        "        # Setup output paths\n",
        "        if output_dir is None:\n",
        "            output_dir = input_path.parent\n",
        "        else:\n",
        "            output_dir = Path(output_dir)\n",
        "            output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        base_name = input_path.stem\n",
        "        output_pdf = output_dir / f\"{base_name}_reordered.pdf\"\n",
        "        report_path = output_dir / f\"{base_name}_analysis_report.json\"\n",
        "\n",
        "        try:\n",
        "            # Step 1: Analyze PDF\n",
        "            self.logger.info(\"Step 1: Analyzing PDF structure...\")\n",
        "            page_mapping = self.analyze_pdf(input_path)\n",
        "\n",
        "            if not page_mapping:\n",
        "                raise ValueError(\"No page numbers could be detected and mapped\")\n",
        "\n",
        "            # Step 2: Create reordered PDF\n",
        "            self.logger.info(\"Step 2: Creating reordered PDF...\")\n",
        "            pages_reordered = self.create_reordered_pdf(input_path, output_pdf, page_mapping)\n",
        "\n",
        "            # Step 3: Generate report\n",
        "            self.logger.info(\"Step 3: Generating analysis report...\")\n",
        "            self.generate_report(report_path, page_mapping)\n",
        "\n",
        "            # Summary\n",
        "            self.logger.info(\"Processing complete!\")\n",
        "            self.logger.info(f\"Input: {input_path}\")\n",
        "            self.logger.info(f\"Output: {output_pdf}\")\n",
        "            self.logger.info(f\"Report: {report_path}\")\n",
        "            self.logger.info(f\"Pages reordered: {pages_reordered}\")\n",
        "\n",
        "            return {\n",
        "                'success': True,\n",
        "                'input_path': str(input_path),\n",
        "                'output_pdf': str(output_pdf),\n",
        "                'report_path': str(report_path),\n",
        "                'pages_reordered': pages_reordered,\n",
        "                'page_mapping': page_mapping\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Processing failed: {e}\")\n",
        "            return {\n",
        "                'success': False,\n",
        "                'error': str(e),\n",
        "                'input_path': str(input_path)\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Adaptive PDF Page Reorderer')\n",
        "    parser.add_argument('input_pdf', help='Path to input PDF file')\n",
        "    parser.add_argument('-o', '--output-dir', help='Output directory (default: same as input)')\n",
        "    parser.add_argument('--no-gpu', action='store_true', help='Disable GPU acceleration')\n",
        "    parser.add_argument('--workers', type=int, help='Number of worker threads')\n",
        "    parser.add_argument('-v', '--verbose', action='store_true', help='Verbose logging')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "\n",
        "    # Initialize reorderer\n",
        "    reorderer = AdaptivePDFReorderer(\n",
        "        gpu_optimization=not args.no_gpu,\n",
        "        max_workers=args.workers\n",
        "    )\n",
        "\n",
        "    # Process PDF\n",
        "    result = reorderer.process_pdf(args.input_pdf, args.output_dir)\n",
        "\n",
        "    if result['success']:\n",
        "        print(f\"SUCCESS: PDF reordered successfully\")\n",
        "        print(f\"Output: {result['output_pdf']}\")\n",
        "        print(f\"Pages reordered: {result['pages_reordered']}\")\n",
        "    else:\n",
        "        print(f\"FAILED: {result['error']}\")\n",
        "        exit(1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For direct usage without command line\n",
        "    if len(os.sys.argv) == 1:\n",
        "        # Example usage - replace with your PDF path\n",
        "        input_pdf = input(\"Enter PDF path: \").strip().strip('\"\\'')\n",
        "\n",
        "        reorderer = AdaptivePDFReorderer(gpu_optimization=True)\n",
        "        result = reorderer.process_pdf(input_pdf)\n",
        "\n",
        "        if result['success']:\n",
        "            print(f\"\\nSUCCESS!\")\n",
        "            print(f\"Reordered PDF: {result['output_pdf']}\")\n",
        "            print(f\"Analysis report: {result['report_path']}\")\n",
        "        else:\n",
        "            print(f\"FAILED: {result['error']}\")\n",
        "    else:\n",
        "        main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "8bDXKpvo5I58",
        "outputId": "d468a3ca-1987-459a-8380-ceeb026025d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [-o OUTPUT_DIR] [--no-gpu]\n",
            "                                [--workers WORKERS] [-v]\n",
            "                                input_pdf\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}